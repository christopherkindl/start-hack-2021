{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full output in Notebook, instead of only the last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# models\n",
    "from pmdarima import auto_arima\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../02_research_paper/00_data/'\n",
    "\n",
    "df_rapperswil = pd.read_csv(os.path.join(data_path, \"features_rapperswil.csv\"), sep=\",\")\n",
    "df_burgdorf = pd.read_csv(os.path.join(data_path, \"features_burgdorf.csv\"), sep=\",\")\n",
    "\n",
    "print('Dataset shape of Rapperswil data: {}'.format(df_rapperswil.shape))\n",
    "print('Dataset shape of Burgdorf data: {}'.format(df_burgdorf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only target variable and date as index for arima\n",
    "# df_rapperswil = df_rapperswil[['date', 'occupancy_rate']]\n",
    "\n",
    "# only keep hour and target variable for heuristic model\n",
    "df_rapperswil = df_rapperswil[['date', 'hour', 'occupancy_rate']]\n",
    "df_burgdorf = df_burgdorf[['date', 'hour', 'occupancy_rate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook is written for using one parking site data at once. Consequently, do a new notebook run for the second parking site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, split_date):\n",
    "    \n",
    "    # define split date\n",
    "    # split_date = datetime.strptime(split_date, '%Y-%m-%d %H:%M:%S').date()\n",
    "    split_date = datetime.strptime(split_date, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # split df into train and test set\n",
    "    df_train = df.loc[df['date'] <= split_date].copy()\n",
    "    df_test = df.loc[df['date'] > split_date].copy()\n",
    "    \n",
    "    # set date as index in both sets\n",
    "    df_train = df_train.set_index('date')\n",
    "    df_test = df_test.set_index('date')\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rapperswil['date'] = pd.to_datetime(df_rapperswil['date'])\n",
    "df_burgdorf['date'] = pd.to_datetime(df_burgdorf['date'])\n",
    "\n",
    "df_train, df_test = split(df_burgdorf, '2021-08-01 01:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Heuristic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the data exploration part, parking occupancy follows a relatively clear 24-hour cyclical pattern with some variance. Therefore, an appealing basemodel would be to consider the average parking occupancy rate for every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make prediction \n",
    "prediction = pd.DataFrame(df_train.groupby('hour')['occupancy_rate'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "df_all = pd.merge(df_test, prediction, on=['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply evaluation metrics\n",
    "print('MAE: ', round(mean_absolute_error(y_true=df_all['occupancy_rate_x'], y_pred=df_all['occupancy_rate_y']), 2))\n",
    "print('RMSE: ', round(mean_squared_error(y_true=df_all['occupancy_rate_x'], y_pred=df_all['occupancy_rate_y'], squared=False), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean absolute error** \\\n",
    "MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
    "\n",
    "**Root mean squared error** \\\n",
    "RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation.\n",
    "\n",
    "**Difference of these two measures** \\\n",
    "Taking the square root of the average squared errors has some interesting implications for RMSE. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable. The three tables below show examples where MAE is steady and RMSE increases as the variance associated with the frequency distribution of error magnitudes also increases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
