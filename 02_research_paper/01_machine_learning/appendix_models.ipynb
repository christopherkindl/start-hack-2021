{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "Write description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dependencies'></a>\n",
    "\n",
    "## 1. Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries might require pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# machine learning libraries\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full output in Notebook, instead of only the last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the locations to be used.\n",
    "locations = ['burgdorf', 'rapperswil']\n",
    "\n",
    "# Set this to False if another grid-search should be preformed.\n",
    "use_default = True\n",
    "\n",
    "# Set this to False if no tex environment is installed.\n",
    "use_tex = True\n",
    "\n",
    "# Set this to True if plots should be saved to disk.\n",
    "save_figs = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterset for iterative results (ONLY RUN ONCE!)\n",
    "\n",
    "prediction_params = {\n",
    "    \"12h\": {\n",
    "        \"horizon\": 12,\n",
    "        \"time\": \"12 hours\",\n",
    "        \"label\": \"+12\",\n",
    "        \"features\": ['hour','day_of_week','quarter','month','day_of_year','day_of_month','week_of_year','holiday',\n",
    "                    'occupancy_rate',\n",
    "                    'temperature',\n",
    "                    'temperature+12',\n",
    "                    'weather',\n",
    "                    'weather+12',\n",
    "                    ]\n",
    "    },\n",
    "    \"24h\": {\n",
    "        \"horizon\": 24,\n",
    "        \"time\": \"24 hours\",\n",
    "        \"label\": \"+24\",\n",
    "        \"features\": ['hour','day_of_week','quarter','month','day_of_year','day_of_month','week_of_year','holiday',\n",
    "                    'temperature+24',\n",
    "                    'weather+24',\n",
    "                    ]\n",
    "    },\n",
    "    \"7d\": {\n",
    "        \"horizon\": 168,\n",
    "        \"time\": \"7 days\",\n",
    "        \"label\": \"+168\",\n",
    "        \"features\": ['hour','day_of_week','quarter','month','day_of_year','day_of_month','week_of_year','holiday',\n",
    "                    'temperature+168',\n",
    "                    'weather+168',\n",
    "                    ]\n",
    "    },\n",
    "    \"14d\": {\n",
    "        \"horizon\": 336,\n",
    "        \"time\": \"14 days\",\n",
    "        \"label\": \"+336\",\n",
    "        \"features\": ['hour','day_of_week','quarter','month','day_of_year','day_of_month','week_of_year','holiday',\n",
    "                    'temperature+336',\n",
    "                    'weather+336',\n",
    "                    ]\n",
    "    },\n",
    "    \"28d\": {\n",
    "        \"horizon\": 672,\n",
    "        \"time\": \"28 days\",\n",
    "        \"label\": \"+672\",\n",
    "        \"features\": ['hour','day_of_week','quarter','month','day_of_year','day_of_month','week_of_year','holiday',\n",
    "                    'temperature+672',\n",
    "                    'weather+672',\n",
    "                    ]\n",
    "    },\n",
    "    \"90d\": {\n",
    "        \"horizon\": 2160,\n",
    "        \"time\": \"90 days\",\n",
    "        \"label\": \"+2160\",\n",
    "        \"features\": ['hour','day_of_week','quarter','month','day_of_year','day_of_month','week_of_year','holiday',\n",
    "                    'temperature+2160',\n",
    "                    'weather+2160',\n",
    "                    ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select horizon from [12h, 24h, 7d, 14d, 28d, 90d]\n",
    "horizon = \"12h\"\n",
    "\n",
    "prediction_param = prediction_params[horizon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a prediction horizon in the following cell. Repeat from there with all keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_import'></a>\n",
    "\n",
    "## 2. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../00_data'\n",
    "\n",
    "df_rapperswil = pd.read_csv(os.path.join(data_path, \"features_rapperswil.csv\"), sep=\",\")\n",
    "df_burgdorf = pd.read_csv(os.path.join(data_path, \"features_burgdorf.csv\"), sep=\",\")\n",
    "\n",
    "print('Dataset shape of Rapperswil data: {}'.format(df_rapperswil.shape))\n",
    "print('Dataset shape of Burgdorf data: {}'.format(df_burgdorf.shape))\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for loc in locations:\n",
    "    dfs[loc] = pd.read_csv(os.path.join(data_path, \"features_{}.csv\".format(loc)), sep=\",\")\n",
    "    dfs[loc]['date'] = pd.to_datetime(dfs[loc]['date'])\n",
    "\n",
    "    #  declare categorical columns\n",
    "    for col in ['hour', 'day_of_week', 'quarter', 'month', 'day_of_year', 'day_of_month',\n",
    "                'week_of_year', 'weather', 'weather_t-1', 'weather_t-2', 'weather_t-3', 'weather_t-7', \n",
    "                'holiday']:\n",
    "        \n",
    "        dfs[loc][col] = dfs[loc][col].astype(object)\n",
    "\n",
    "    # set datetime column as index\n",
    "    dfs[loc].set_index('date', inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the long-term predictions, shift the labels by the specified amount of time and adapt feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    dfs[loc]['occupancy{}'.format(prediction_param['label'])] = dfs[loc]['occupancy_rate']\n",
    "    dfs[loc]['temperature{}'.format(prediction_param['label'])] = dfs[loc]['temperature']\n",
    "    dfs[loc]['weather{}'.format(prediction_param['label'])] = dfs[loc]['weather']\n",
    "    dfs[loc]['occupancy{}'.format(prediction_param['label'])] = dfs[loc]['occupancy{}'.format(prediction_param['label'])].shift(-prediction_param['horizon'])\n",
    "    dfs[loc]['temperature{}'.format(prediction_param['label'])] = dfs[loc]['temperature{}'.format(prediction_param['label'])].shift(-prediction_param['horizon'])\n",
    "    dfs[loc]['weather{}'.format(prediction_param['label'])] = dfs[loc]['weather{}'.format(prediction_param['label'])].shift(-prediction_param['horizon'])\n",
    "    dfs[loc] = dfs[loc].iloc[:-prediction_param['horizon']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracts a feature column from the dataframe.\\n\",\n",
    "# @param df         the dataframe.\n",
    "# @param features   the features to be included.\n",
    "# @param label      the label of the column to be extracted.\n",
    "#\n",
    "# @returns the original dataframe and (if found) the column indexed by @p label or (if not found) None.\n",
    "def extract_features(df:pd.DataFrame, features:list, label:str=None):\n",
    "    X = df[features]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print('Dataset shape of {} data: {}'.format(loc.capitalize(), dfs[loc].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_preprocessing'></a>\n",
    "\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "Split datasets into train and test data using 70/30% ratio by considering the order of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify split date manually and use beginning of the month of the specific date\n",
    "# e.g. 2021-08-16 04:00:00 --> 2021-08-01 01:00:00, only for pragmatic reasons for better paper story\n",
    "splits = {}\n",
    "\n",
    "for loc in locations:\n",
    "    splits[loc] = 0.7 * len(dfs[loc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splits a datetime-indexed dataframe according to a specified split date.\n",
    "# @param df         a datetime-indexed dataframe.\n",
    "# @param split_date the split date.\n",
    "#\n",
    "# @returns A copy of the lower part of the dataframe (including split date) and a copy of the upper part.\n",
    "\n",
    "def split(df:pd.DataFrame, split_date:pd.DatetimeIndex):\n",
    "    # split df into train and test set\n",
    "    df_train = df.loc[df.index <= split_date].copy()\n",
    "    df_test = df.loc[df.index > split_date].copy()\n",
    "    \n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train, dfs_test = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    dfs_train[loc], dfs_test[loc] = split(dfs[loc], dfs[loc].index[int(splits[loc])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print('Cutoff date {}:\\t{}\\t({}/{} entries)'.format(loc.capitalize(), dfs[loc].index[int(splits[loc])], len(dfs_train[loc]), len(dfs_test[loc])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the occupancy rate as a label\n",
    "X_train, X_test = {}, {}\n",
    "y_train, y_test = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    X_train[loc], y_train[loc] = extract_features(dfs_train[loc], prediction_param['features'], label='occupancy{}'.format(prediction_param['label']))\n",
    "    X_test[loc], y_test[loc] = extract_features(dfs_test[loc], prediction_param['features'], label='occupancy{}'.format(prediction_param['label']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature standardization and scaling\n",
    "\n",
    "Features vary in magnitude and units, which is why feature scaling is applied, using `StandardScaler()` for numeric features and `OneHotEncoder()` for categorical features. For example, the input value `day_of_week` should not be used as a continuous value from 1 to 7, since this would associate a higher weight to the later weekdays (5, 6 or 7) than to the earlier ones (1,2 and 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardize features using standard scaling and one-hot encoding.\n",
    "# @p df_train   the training dataframe.\n",
    "# @p df_test    the test dataframe.\n",
    "#\n",
    "# @returns  the standardized training and test datasets, as well as the generated pipeline object.\n",
    "def standardize_features(df_train:pd.DataFrame, df_test:pd.DataFrame):\n",
    "    # split numerical and categorical columns\n",
    "    data_num = df_train.select_dtypes(include=[np.number])\n",
    "    data_cat = df_train.select_dtypes(include=[object])\n",
    "\n",
    "    # check whether no columns got lost during type allocation\n",
    "    len(df_train.columns) == len(data_num.columns) + len(data_cat.columns)\n",
    "\n",
    "    # create data pipeline\n",
    "    num_pipeline = Pipeline([('std_scaler', StandardScaler())])\n",
    "\n",
    "    num_attribs = list(data_num)\n",
    "    cat_attribs = list(data_cat)\n",
    "\n",
    "    full_pipeline = ColumnTransformer([\n",
    "            ('num', num_pipeline, num_attribs),\n",
    "            # don’t take precautions to handle unseen values for OneHotEncoder\n",
    "            ('cat', OneHotEncoder(handle_unknown = 'ignore'), cat_attribs)])\n",
    "\n",
    "    # fit and transform training data set with preprocessing pipeline and\n",
    "    # only transform test feature set with the pipeline fit on training feature set to not\n",
    "    # artificially improve test performance\n",
    "\n",
    "    return full_pipeline.fit_transform(df_train), full_pipeline.transform(df_test), full_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {}\n",
    "\n",
    "for loc in locations:\n",
    "        X_train[loc], X_test[loc], pipelines[loc] = standardize_features(X_train[loc], X_test[loc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_training'></a>\n",
    "\n",
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameter optimization, we apply `random grid search` with cross validation to narrow down the range of reasonable values for the given parameters for the models. Then, `full grid search` with cross validation is applied with the value range obtained from the random grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Define random grid and run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start, t_end, params = {}, {}, {}\n",
    "\n",
    "if not use_default:\n",
    "    # number of trees in random forest\n",
    "    n_estimators = [x for x in range(200, 2000, 25)]\n",
    "\n",
    "    # learning rate\n",
    "    arr = np.arange(0.01, 1.0, 0.05)\n",
    "    eta = arr.tolist()\n",
    "\n",
    "    # defines the minimum sum of weights of all observations required in a child\n",
    "    min_child_weight = [i for i in range(1, 100, 8)]\n",
    "\n",
    "    # maximum depth of a tree\n",
    "    max_depth = [i for i in range(1, 250, 5)]\n",
    "\n",
    "    # regularization\n",
    "    gamma = [0, 0.5, 1, 3, 5]\n",
    "\n",
    "    # number of columns used by each tree\n",
    "    colsample_bytree = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "    # maximum number of leaf nodes in tree\n",
    "    max_leaf_nodes = [i for i in range(10, 300, 5)]\n",
    "    max_leaf_nodes.append(None)\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {'n_estimators' : n_estimators,\n",
    "                   'eta' : eta,\n",
    "                   'max_depth': max_depth,\n",
    "                # 'max_features': max_features,\n",
    "                   'min_child_weight': min_child_weight,\n",
    "                   'gamma' : gamma,\n",
    "                   'colsample_bytree' : colsample_bytree,\n",
    "                   'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "    for loc in locations:\n",
    "        # create xgb model\n",
    "        regressor_rnd = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "        # ensure prediction is made on subsequent data\n",
    "        cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "        # random search of parameters, using 3 fold cross validation, \n",
    "        # search across 75 different combinations, and use all available cores\n",
    "        xbg_random = RandomizedSearchCV(estimator = regressor_rnd, param_distributions = random_grid,\n",
    "                                        n_iter = 75, cv = cv,\n",
    "                                        verbose=2, random_state=42, n_jobs = -1)\n",
    "        # fit the xgboost model\n",
    "        t_start[loc] = time.time()\n",
    "        xbg_random.fit(X_train[loc], y_train[loc])\n",
    "        t_end[loc] = time.time()\n",
    "\n",
    "        params[loc] = xbg_random.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_default:\n",
    "    for loc in locations:\n",
    "        print(\"Grid-search {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n",
    "        print(\"Best parameters are:\")\n",
    "        params[loc]\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use same procedure to run the `full_grid_search` with the obtained values from `random_grid_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide more granular range based on values from random grid search\n",
    "if not use_default:\n",
    "    for loc in locations:\n",
    "\n",
    "        # number of trees in random forest\n",
    "        n_estimators = [x for x in range(max(params[loc]['n_estimators'] - 25, 1), params[loc]['n_estimators'] + 25, 1)]\n",
    "        \n",
    "        # learning rate\n",
    "        eta = np.array([params[loc]['eta']]).tolist()\n",
    "\n",
    "        # defines the minimum sum of weights of all observations required in a child\n",
    "        min_child_weight = [i for i in range(max(params[loc]['min_child_weight'] - 8, 1), params[loc]['min_child_weight'] + 8, 1)]\n",
    "\n",
    "        # maximum depth of a tree\n",
    "        max_depth = [i for i in range(max(params[loc]['max_depth'] - 5, 1), params[loc]['max_depth'] + 5, 1)]\n",
    "        \n",
    "        # regularization\n",
    "        gamma = [0, 1, 5]\n",
    "        \n",
    "        # number of columns used by each tree\n",
    "        colsample_bytree = [0.6, 0.7, 0.8, 0.9, 1]\n",
    "        \n",
    "        # maximum number of leaf nodes in tree\n",
    "        max_leaf_nodes = [i for i in range(max(params[loc]['max_depth'] - 5, 1), params[loc]['max_depth'] + 5, 1)]\n",
    "        max_leaf_nodes.append(None)\n",
    "        \n",
    "        # create random grid\n",
    "        random_grid = {'n_estimators' : n_estimators,\n",
    "            'eta' : eta,\n",
    "            'max_depth': max_depth,\n",
    "            # 'max_features': max_features,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'gamma' : gamma,\n",
    "            'colsample_bytree' : colsample_bytree,\n",
    "            'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "        # create xgb model\n",
    "        regressor_rnd = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "        # ensure prediction is made on subsequent data\n",
    "        cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "        # random search of parameters, using 3 fold cross validation, \n",
    "        # search across 75 different combinations, and use all available cores\n",
    "        xbg_random = RandomizedSearchCV(estimator = regressor_rnd, param_distributions = random_grid,\n",
    "                                        n_iter = 75, cv = cv,\n",
    "                                        verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "        # xbg_random = GridSearchCV(estimator = regressor_rnd, param_distributions = random_grid,\n",
    "        #                                 n_iter = 75, cv = cv,\n",
    "        #                                 verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "        # fit the xgboost model\n",
    "        t_start[loc] = time.time()\n",
    "        xbg_random.fit(X_train[loc], y_train[loc])\n",
    "        t_end[loc] = time.time()\n",
    "\n",
    "        params[loc] = xbg_random.best_params_\n",
    "\n",
    "else:\n",
    "    params['burgdorf'] = {\n",
    "        'n_estimators': 225,\n",
    "        'min_child_weight': 17,\n",
    "        'max_leaf_nodes': 285,\n",
    "        'max_depth': 241,\n",
    "        'gamma': 0.5,\n",
    "        'eta': 0.16000000000000003,\n",
    "        'colsample_bytree': 0.7\n",
    "    }\n",
    "        \n",
    "    params['rapperswil'] = {\n",
    "        'n_estimators': 550,\n",
    "        'min_child_weight': 73,\n",
    "        'max_leaf_nodes': 25,\n",
    "        'max_depth': 36,\n",
    "        'gamma': 1,\n",
    "        'eta': 0.21000000000000002,\n",
    "        'colsample_bytree': 0.5\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_default:\n",
    "    for loc in locations:\n",
    "        print(\"Grid-search {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n",
    "        print(\"Best parameters are:\")\n",
    "        params[loc]\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {}\n",
    "\n",
    "for loc in locations:\n",
    "    regressors[loc] = xgb.XGBRegressor(**params[loc], random_state=42)\n",
    "\n",
    "    # fit the random search model\n",
    "    t_start[loc] = time.time()\n",
    "    _ = regressors[loc].fit(X_train[loc], y_train[loc])\n",
    "    t_end[loc] = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print(\"Fitting for {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes, rmses = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    t_start[loc] = time.time()\n",
    "    dfs_test[loc]['pred_xgb_all_features'] = regressors[loc].predict(X_test[loc])\n",
    "    t_end[loc] = time.time()\n",
    "    maes[loc] = mean_absolute_error(y_true=dfs_test[loc]['occupancy{}'.format(prediction_param['label'])], y_pred=dfs_test[loc]['pred_xgb_all_features'])\n",
    "    rmses[loc] = mean_squared_error(y_true=dfs_test[loc]['occupancy{}'.format(prediction_param['label'])], y_pred=dfs_test[loc]['pred_xgb_all_features'], squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print(\"Prediction for {} ({} entries) took {} seconds.\".format(loc.capitalize(), len(dfs_test[loc]), round(t_end[loc] - t_start[loc], 2)))\n",
    "    print(\"\\tMAE: \", round(maes[loc], 2))\n",
    "    print(\"\\tRMSE: \", round(rmses[loc], 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_params = {\n",
    "    'text.usetex' : use_tex,\n",
    "    'font.size' : 20,\n",
    "    'xtick.labelsize' : 18,\n",
    "    'ytick.labelsize' : 18,\n",
    "    'lines.linewidth': 1,\n",
    "    'grid.linewidth':   2,\n",
    "}\n",
    "plt.rcParams.update(plt_params)\n",
    "\n",
    "for loc in ['burgdorf', 'rapperswil']:\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "    # plot the true target values for the test set versus the estimated values with the best model\n",
    "    _ = plt.scatter(dfs_test[loc]['occupancy{}'.format(prediction_param['label'])], \n",
    "        dfs_test[loc]['pred_xgb_all_features'],\n",
    "        alpha=0.1);\n",
    "    _ = plt.plot([0, 100], [0, 100], \"k--\", lw=3);\n",
    "    _ = plt.xlabel(r'Measured parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.ylabel(r'Predicted parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.title('Prediction for {} days: True values vs predicted values ({})'.format(prediction_param['time'], loc.capitalize()));\n",
    "    _ = plt.grid(None)\n",
    "\n",
    "    # provide MAE and RMSE details to the plot\n",
    "    textstr = '\\n'.join((\n",
    "        r'MAE: %.2f' % (maes[loc]),\n",
    "        r'RMSE: %.2f' % (rmses[loc])\n",
    "    ))\n",
    "\n",
    "    # style infobox\n",
    "    # plt.gca().hist(x, 20)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    _ = plt.text(0.03, 0.95, textstr, verticalalignment='top', bbox=props, transform=plt.gca().transAxes)\n",
    "\n",
    "    if save_figs:\n",
    "        # export - pay attention to an appropriate name\n",
    "        filename = 'pred_' + loc + prediction_param['label'] + '.png'\n",
    "        plt.savefig('../02_results/' + filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_params[horizon]['results'] = {}\n",
    "\n",
    "for loc in locations:\n",
    "    df_error = round(dfs_test[loc]['occupancy{}'.format(prediction_param['label'])] - dfs_test[loc]['pred_xgb_all_features'], 2)\n",
    "\n",
    "    # set size of figure\n",
    "    fig = plt.figure(figsize=(20,12));\n",
    "    ax = sns.histplot(df_error)\n",
    "\n",
    "    _ = plt.xlabel(r'Prediction Error $\\left[\\%\\right]$');\n",
    "    _ = plt.ylabel(r'Number of occurences');\n",
    "    _ = plt.title(r'Prediction for ' + prediction_param['time'] + r': $\\hat{y} - y$ (' + loc.capitalize() + ')');\n",
    "    _ = plt.grid(None)\n",
    "\n",
    "    textstr = '\\n'.join((\n",
    "        r'$\\mu:$ ' + str(round(np.mean(df_error), 2)),\n",
    "        r'$\\sigma:$ ' + str(round(np.std(df_error), 2))\n",
    "    ))\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    _ = plt.text(0.03, 0.95, textstr, verticalalignment='top', bbox=props, transform=plt.gca().transAxes)\n",
    "\n",
    "    prediction_params[horizon]['results'][loc] = df_error\n",
    "\n",
    "    # save fig\n",
    "    if save_figs:\n",
    "        filename = 'hist_' + loc + prediction_param['label'] + '.png'\n",
    "        plt.savefig('../02_results/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors = {}\n",
    "\n",
    "for loc in locations:\n",
    "\n",
    "    df_errors[loc] = pd.DataFrame(columns=prediction_params.keys())\n",
    "    for hor in prediction_params.keys():\n",
    "        df_errors[loc][hor] = prediction_params[hor]['results'][loc]\n",
    "\n",
    "    # set size of figure\n",
    "    fig = plt.figure(figsize=(20,12));\n",
    "\n",
    "    ax = sns.boxplot(data=df_errors[loc], linewidth=2, color=\"#B4D0E4\", boxprops=dict(alpha=.7));\n",
    "\n",
    "    x_labels = []\n",
    "    for hor in prediction_params.keys():\n",
    "        x_labels.append(prediction_params[hor]['time'])\n",
    "\n",
    "    _ = ax.set_xticklabels(x_labels)\n",
    "\n",
    "    _ = plt.title(r'Boxplot of Prediction Error $\\hat{y} - y$ ' + '({})'.format(loc.capitalize()))    \n",
    "    _ = plt.xlabel(r'Prediction Horizon');\n",
    "    _ = plt.ylabel(r'Prediction Error');\n",
    "\n",
    "    # save fig\n",
    "    if save_figs:\n",
    "        filename = 'box_' + loc + prediction_param['label'] + '.png'\n",
    "        plt.savefig('../02_results/' + filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
