{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Models\n",
    "\n",
    "This IPython notebook contains the entire source code for recreating the advanced model (gradient boosted regressor) as discussed in the work _Predictive Models for Parking Space Occupation_. Please note that running the script `feature_engineering` is a prerequisite to run this notebook.\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "1. [Dependencies](#dependencies)\n",
    "2. [Data Import](#data_import)\n",
    "3. [Data Preprocessing](#data_preprocessing)\n",
    "4. [Model Training](#model_training)\n",
    "5. [Appendix](#appendix)\n",
    "    1. [Single Model](#single_model)\n",
    "    2. [Rolling Predictions](#rolling_predictions)\n",
    "    3. [Random Forest](#rnd_forest)\n",
    "\n",
    "For the purpose of legibility, code cells are separated, where applicable, to reflect segments required only when running the notebook interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dependencies'></a>\n",
    "\n",
    "## 1. Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries might require pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# machine learning libraries\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full output in Notebook, instead of only the last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the locations to be used.\n",
    "locations = ['burgdorf', 'rapperswil']\n",
    "\n",
    "# Set this to False if another grid-search should be preformed.\n",
    "use_default = True\n",
    "\n",
    "# Set this to False if no tex environment is installed.\n",
    "use_tex = True\n",
    "\n",
    "# Set this to True if plots should be saved to disk.\n",
    "save_figs = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_import'></a>\n",
    "\n",
    "## 2. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape of Rapperswil data: (12303, 30)\n",
      "Dataset shape of Burgdorf data: (7768, 30)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../00_data'\n",
    "\n",
    "df_rapperswil = pd.read_csv(os.path.join(data_path, \"features_rapperswil.csv\"), sep=\",\")\n",
    "df_burgdorf = pd.read_csv(os.path.join(data_path, \"features_burgdorf.csv\"), sep=\",\")\n",
    "\n",
    "print('Dataset shape of Rapperswil data: {}'.format(df_rapperswil.shape))\n",
    "print('Dataset shape of Burgdorf data: {}'.format(df_burgdorf.shape))\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for loc in locations:\n",
    "    dfs[loc] = pd.read_csv(os.path.join(data_path, \"features_{}.csv\".format(loc)), sep=\",\")\n",
    "    dfs[loc]['date'] = pd.to_datetime(dfs[loc]['date'])\n",
    "\n",
    "    #  declare categorical columns\n",
    "    for col in ['hour', 'day_of_week', 'quarter', 'month', 'day_of_year', 'day_of_month',\n",
    "                'week_of_year', 'weather', 'weather_t-1', 'weather_t-2', 'weather_t-3', 'weather_t-7', \n",
    "                'holiday']:\n",
    "        \n",
    "        dfs[loc][col] = dfs[loc][col].astype(object)\n",
    "\n",
    "    # set datetime column as index\n",
    "    dfs[loc].set_index('date', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print('Dataset shape of {} data: {}'.format(loc.capitalize(), dfs[loc].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_preprocessing'></a>\n",
    "\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "Split datasets into train and test data using 70/30% ratio by considering the order of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify split date manually and use beginning of the month of the specific date\n",
    "# e.g. 2021-08-16 04:00:00 --> 2021-08-01 01:00:00, only for pragmatic reasons for better paper story\n",
    "splits = {}\n",
    "\n",
    "for loc in locations:\n",
    "    splits[loc] = 0.7 * len(dfs[loc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splits a datetime-indexed dataframe according to a specified split date.\n",
    "# @param df         a datetime-indexed dataframe.\n",
    "# @param split_date the split date.\n",
    "#\n",
    "# @returns A copy of the lower part of the dataframe (including split date) and a copy of the upper part.\n",
    "\n",
    "def split(df:pd.DataFrame, split_date:pd.DatetimeIndex):\n",
    "    # split df into train and test set\n",
    "    df_train = df.loc[df.index <= split_date].copy()\n",
    "    df_test = df.loc[df.index > split_date].copy()\n",
    "    \n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train, dfs_test = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    dfs_train[loc], dfs_test[loc] = split(dfs[loc], dfs[loc].index[int(splits[loc])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print('Cutoff date {}:\\t{}\\t({}/{} entries)'.format(loc.capitalize(), dfs[loc].index[int(splits[loc])], len(dfs_train[loc]), len(dfs_test[loc])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracts a feature column from the dataframe.\\n\",\n",
    "# @param df     the dataframe.\n",
    "# @param label  the label of the column to be extracted.\n",
    "#\n",
    "# @returns the original dataframe and (if found) the column indexed by @p label or (if not found) None.\n",
    "def extract_features(df:pd.DataFrame, label:str=None):\n",
    "    X = df[[# 'date_only', (is not as a feature and kept out-commented to have overview of source data)\n",
    "             'hour',\n",
    "             'day_of_week',\n",
    "             'quarter',\n",
    "             'month',\n",
    "             'day_of_year',\n",
    "             'day_of_month',\n",
    "             'week_of_year',\n",
    "             #'temperature',\n",
    "             'temperature_t-1',\n",
    "             'temperature_t-2',\n",
    "             'temperature_t-3',\n",
    "             'temperature_t-7',\n",
    "             'weather',\n",
    "             'weather_t-1',\n",
    "             'weather_t-2',\n",
    "             'weather_t-3',\n",
    "             'weather_t-7',\n",
    "             'holiday',\n",
    "             't-7',\n",
    "             't-3',\n",
    "             't-2',\n",
    "             't-1']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the occupancy rate as a label\n",
    "X_train, X_test = {}, {}\n",
    "y_train, y_test = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    X_train[loc], y_train[loc] = extract_features(dfs_train[loc], label='occupancy_rate')\n",
    "    X_test[loc], y_test[loc] = extract_features(dfs_test[loc], label='occupancy_rate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature standardization and scaling\n",
    "\n",
    "Features vary in magnitude and units, which is why feature scaling is applied, using `StandardScaler()` for numeric features and `OneHotEncoder()` for categorical features. For example, the input value `day_of_week` should not be used as a continuous value from 1 to 7, since this would associate a higher weight to the later weekdays (5, 6 or 7) than to the earlier ones (1,2 and 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardize features using standard scaling and one-hot encoding.\n",
    "# @p df_train   the training dataframe.\n",
    "# @p df_test    the test dataframe.\n",
    "#\n",
    "# @returns  the standardized training and test datasets, as well as the generated pipeline object.\n",
    "def standardize_features(df_train:pd.DataFrame, df_test:pd.DataFrame):\n",
    "    # split numerical and categorical columns\n",
    "    data_num = df_train.select_dtypes(include=[np.number])\n",
    "    data_cat = df_train.select_dtypes(include=[object])\n",
    "\n",
    "    # check whether no columns got lost during type allocation\n",
    "    len(df_train.columns) == len(data_num.columns) + len(data_cat.columns)\n",
    "\n",
    "    # create data pipeline\n",
    "    num_pipeline = Pipeline([('std_scaler', StandardScaler())])\n",
    "\n",
    "    num_attribs = list(data_num)\n",
    "    cat_attribs = list(data_cat)\n",
    "\n",
    "    full_pipeline = ColumnTransformer([\n",
    "            ('num', num_pipeline, num_attribs),\n",
    "            # don’t take precautions to handle unseen values for OneHotEncoder\n",
    "            ('cat', OneHotEncoder(handle_unknown = 'ignore'), cat_attribs)])\n",
    "\n",
    "    # fit and transform training data set with preprocessing pipeline and\n",
    "    # only transform test feature set with the pipeline fit on training feature set to not\n",
    "    # artificially improve test performance\n",
    "\n",
    "    return full_pipeline.fit_transform(df_train), full_pipeline.transform(df_test), full_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {}\n",
    "\n",
    "for loc in locations:\n",
    "        X_train[loc], X_test[loc], pipelines[loc] = standardize_features(X_train[loc], X_test[loc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_training'></a>\n",
    "\n",
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameter optimization, we apply `random grid search` with cross validation to narrow down the range of reasonable values for the given parameters for the models. Then, `full grid search` with cross validation is applied with the value range obtained from the random grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Define random grid and run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start, t_end, params = {}, {}, {}\n",
    "\n",
    "if not use_default:\n",
    "    # number of trees in random forest\n",
    "    n_estimators = [x for x in range(200, 2000, 25)]\n",
    "\n",
    "    # learning rate\n",
    "    arr = np.arange(0.01, 1.0, 0.05)\n",
    "    eta = arr.tolist()\n",
    "\n",
    "    # defines the minimum sum of weights of all observations required in a child\n",
    "    min_child_weight = [i for i in range(1, 100, 8)]\n",
    "\n",
    "    # maximum depth of a tree\n",
    "    max_depth = [i for i in range(1, 250, 5)]\n",
    "\n",
    "    # regularization\n",
    "    gamma = [0, 0.5, 1, 3, 5]\n",
    "\n",
    "    # number of columns used by each tree\n",
    "    colsample_bytree = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "    # maximum number of leaf nodes in tree\n",
    "    max_leaf_nodes = [i for i in range(10, 300, 5)]\n",
    "    max_leaf_nodes.append(None)\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {'n_estimators' : n_estimators,\n",
    "                   'eta' : eta,\n",
    "                   'max_depth': max_depth,\n",
    "                # 'max_features': max_features,\n",
    "                   'min_child_weight': min_child_weight,\n",
    "                   'gamma' : gamma,\n",
    "                   'colsample_bytree' : colsample_bytree,\n",
    "                   'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "    for loc in locations:\n",
    "        # create xgb model\n",
    "        regressor_rnd = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "        # ensure prediction is made on subsequent data\n",
    "        cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "        # random search of parameters, using 3 fold cross validation, \n",
    "        # search across 75 different combinations, and use all available cores\n",
    "        xbg_random = RandomizedSearchCV(estimator = regressor_rnd, param_distributions = random_grid,\n",
    "                                        n_iter = 75, cv = cv,\n",
    "                                        verbose=2, random_state=42, n_jobs = -1)\n",
    "        # fit the xgboost model\n",
    "        t_start[loc] = time.time()\n",
    "        xbg_random.fit(X_train[loc], y_train[loc])\n",
    "        t_end[loc] = time.time()\n",
    "\n",
    "        params[loc] = xbg_random.best_params_\n",
    "\n",
    "else:\n",
    "    params['burgdorf'] = {\n",
    "        'n_estimators': 725,\n",
    "        'min_child_weight': 1,\n",
    "        'max_leaf_nodes': 40,\n",
    "        'max_depth': 26,\n",
    "        'gamma': 0,\n",
    "        'eta': 0.11,\n",
    "        'colsample_bytree': 1}\n",
    "        \n",
    "    params['rapperswil'] = {\n",
    "        'n_estimators': 425,\n",
    "        'min_child_weight': 49,\n",
    "        'max_leaf_nodes': 175,\n",
    "        'max_depth': 51,\n",
    "        'gamma': 1,\n",
    "        'eta': 0.01,\n",
    "        'colsample_bytree': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_default:\n",
    "    for loc in locations:\n",
    "        print(\"Grid-search {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n",
    "        print(\"Best parameters are:\")\n",
    "        params[loc]\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use same procedure to run the `full_grid_search` with the obtained values from `random_grid_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide more granular range based on values from random grid search\n",
    "if not use_default:\n",
    "    for loc in locations:\n",
    "\n",
    "        # number of trees in random forest\n",
    "        n_estimators = [x for x in range(max(params[loc]['n_estimators'] - 25, 1), params[loc]['n_estimators'] + 25, 1)]\n",
    "        \n",
    "        # learning rate\n",
    "        eta = np.array([params[loc]['eta']]).tolist()\n",
    "\n",
    "        # defines the minimum sum of weights of all observations required in a child\n",
    "        min_child_weight = [i for i in range(max(params[loc]['min_child_weight'] - 8, 1), params[loc]['min_child_weight'] + 8, 1)]\n",
    "\n",
    "        # maximum depth of a tree\n",
    "        max_depth = [i for i in range(max(params[loc]['max_depth'] - 5, 1), params[loc]['max_depth'] + 5, 1)]\n",
    "        \n",
    "        # regularization\n",
    "        gamma = [0, 1, 5]\n",
    "        \n",
    "        # number of columns used by each tree\n",
    "        colsample_bytree = [0.6, 0.7, 0.8, 0.9, 1]\n",
    "        \n",
    "        # maximum number of leaf nodes in tree\n",
    "        max_leaf_nodes = [i for i in range(max(params[loc]['max_depth'] - 5, 1), params[loc]['max_depth'] + 5, 1)]\n",
    "        max_leaf_nodes.append(None)\n",
    "        \n",
    "        # create random grid\n",
    "        random_grid = {'n_estimators' : n_estimators,\n",
    "            'eta' : eta,\n",
    "            'max_depth': max_depth,\n",
    "            # 'max_features': max_features,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'gamma' : gamma,\n",
    "            'colsample_bytree' : colsample_bytree,\n",
    "            'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "        # create xgb model\n",
    "        regressor_rnd = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "        # ensure prediction is made on subsequent data\n",
    "        cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "        # random search of parameters, using 3 fold cross validation, \n",
    "        # search across 75 different combinations, and use all available cores\n",
    "        xbg_random = RandomizedSearchCV(estimator = regressor_rnd, param_distributions = random_grid,\n",
    "                                        n_iter = 75, cv = cv,\n",
    "                                        verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "        # xbg_random = GridSearchCV(estimator = regressor_rnd, param_distributions = random_grid,\n",
    "        #                                 n_iter = 75, cv = cv,\n",
    "        #                                 verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "        # fit the xgboost model\n",
    "        t_start[loc] = time.time()\n",
    "        xbg_random.fit(X_train[loc], y_train[loc])\n",
    "        t_end[loc] = time.time()\n",
    "\n",
    "        params[loc] = xbg_random.best_params_\n",
    "\n",
    "else:\n",
    "    params['burgdorf'] = {\n",
    "        'n_estimators': 742,\n",
    "        'min_child_weight': 6,\n",
    "        'max_leaf_nodes': 23,\n",
    "        'max_depth': 22,\n",
    "        'gamma': 5,\n",
    "        'eta': 0.11,\n",
    "        'colsample_bytree': 0.9\n",
    "    }\n",
    "        \n",
    "    params['rapperswil'] = {\n",
    "        'n_estimators': 424,\n",
    "        'min_child_weight': 42,\n",
    "        'max_leaf_nodes': 47,\n",
    "        'max_depth': 55,\n",
    "        'gamma': 0,\n",
    "        'eta': 0.01,\n",
    "        'colsample_bytree': 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_default:\n",
    "    for loc in locations:\n",
    "        print(\"Grid-search {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n",
    "        print(\"Best parameters are:\")\n",
    "        params[loc]\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {}\n",
    "\n",
    "for loc in locations:\n",
    "    regressors[loc] = xgb.XGBRegressor(**params[loc], random_state=42)\n",
    "\n",
    "    # fit the random search model\n",
    "    t_start[loc] = time.time()\n",
    "    _ = regressors[loc].fit(X_train[loc], y_train[loc])\n",
    "    t_end[loc] = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print(\"Fitting for {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes, rmses = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    t_start[loc] = time.time()\n",
    "    dfs_test[loc]['pred_xgb_all_features'] = regressors[loc].predict(X_test[loc])\n",
    "    t_end[loc] = time.time()\n",
    "    maes[loc] = mean_absolute_error(y_true=dfs_test[loc]['occupancy_rate'], y_pred=dfs_test[loc]['pred_xgb_all_features'])\n",
    "    rmses[loc] = mean_squared_error(y_true=dfs_test[loc]['occupancy_rate'], y_pred=dfs_test[loc]['pred_xgb_all_features'], squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print(\"Prediction for {} ({} entries) took {} seconds.\".format(loc.capitalize(), len(dfs_test[loc]), round(t_end[loc] - t_start[loc], 2)))\n",
    "    print(\"\\tMAE: \", round(maes[loc], 2))\n",
    "    print(\"\\tRMSE: \", round(rmses[loc], 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_params = {\n",
    "    'text.usetex' : use_tex,\n",
    "    'font.size' : 20,\n",
    "    'xtick.labelsize' : 18,\n",
    "    'ytick.labelsize' : 18,\n",
    "    'lines.linewidth': 1,\n",
    "    'grid.linewidth':   2,\n",
    "}\n",
    "plt.rcParams.update(plt_params)\n",
    "\n",
    "for loc in ['burgdorf', 'rapperswil']:\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "    # plot the true target values for the test set versus the estimated values with the best model\n",
    "    _ = plt.scatter(dfs_test[loc]['occupancy_rate'], \n",
    "        dfs_test[loc]['pred_xgb_all_features'],\n",
    "        alpha=0.1);\n",
    "    _ = plt.plot([0, 100], [0, 100], \"k--\", lw=3);\n",
    "    _ = plt.xlabel(r'Measured parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.ylabel(r'Predicted parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.title('All features: True values vs predicted values ({})'.format(loc.capitalize()));\n",
    "\n",
    "    # provide MAE and RMSE details to the plot\n",
    "    textstr = '\\n'.join((\n",
    "        r'MAE: %.2f' % (maes[loc]),\n",
    "        r'RMSE: %.2f' % (rmses[loc])\n",
    "    ))\n",
    "\n",
    "    # style infobox\n",
    "    # plt.gca().hist(x, 20)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    _ = plt.text(0.03, 0.95, textstr, verticalalignment='top', bbox=props, transform=plt.gca().transAxes)\n",
    "\n",
    "    if save_figs:\n",
    "        # export - pay attention to an appropriate name\n",
    "        filename = 'f_all_' + loc + '.png'\n",
    "        plt.savefig('../05_visualisations_of_eda/' + filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='appendix'></a>\n",
    "## 5. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single_model'></a>\n",
    "\n",
    "### 1. Single Model\n",
    "\n",
    "In the subsequent code section a single model is generated in order to predict occupancies for multiple parking locations. The data is selected such that only entries from a date range present in both datasets are considered. In order to destinguish the data, an additional column `loc` is added, which is a categorical feature and thus one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find overlapping window\n",
    "start_date = max(dfs['burgdorf'].index[0], dfs['rapperswil'].index[0])\n",
    "end_date = min(dfs['burgdorf'].index[-1], dfs['rapperswil'].index[-1])\n",
    "\n",
    "if not (start_date < end_date):\n",
    "    raise AssertionError(\"Non-overlapping time windows!\")\n",
    "\n",
    "# Merge dataframes by time and location name\n",
    "dfcomb = dfs['burgdorf'].loc[(dfs['burgdorf'].index >= start_date) & (dfs['burgdorf'].index <= end_date)].assign(loc='burgdorf')\n",
    "dfcomb = dfcomb.append(dfs['rapperswil'].loc[(dfs['rapperswil'].index >= start_date) & (dfs['rapperswil'].index <= end_date)].assign(loc='rapperswil'))\n",
    "dfcomb.sort_values(['date', 'loc'],inplace=True)\n",
    "\n",
    "print('Merged datasets (total {} entries)'.format(len(dfcomb)))\n",
    "\n",
    "# Perform Train/Test split\n",
    "splitcomb = 0.7 * len(dfcomb)\n",
    "comb_train, comb_test = split(dfcomb, dfcomb.index[int(splitcomb)])\n",
    "print('Cutoff date:\\t{}\\t({}/{} entries)'.format(dfcomb.index[int(splitcomb)], len(comb_train), len(comb_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracts a feature column from the combined dataframe.\n",
    "# @param df     the dataframe.\n",
    "# @param label  the label of the column to be extracted.\n",
    "#\n",
    "# @returns the original dataframe and (if found) the column indexed by @p label or (if not found) None.\n",
    "def extract_combined_features(df:pd.DataFrame, label:str=None):\n",
    "    X = df[[# 'date_only', (is not as a feature and kept out-commented to have overview of source data)\n",
    "             'hour',\n",
    "             'day_of_week',\n",
    "             'quarter',\n",
    "             'month',\n",
    "             'day_of_year',\n",
    "             'day_of_month',\n",
    "             'week_of_year',\n",
    "             'temperature',\n",
    "             'weather',\n",
    "             'holiday',\n",
    "             't-7',\n",
    "             't-3',\n",
    "             't-2',\n",
    "             't-1',\n",
    "             'loc']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combX_train, comby_train = extract_combined_features(comb_train, label='occupancy_rate')\n",
    "combX_test, comby_test = extract_combined_features(comb_test, label='occupancy_rate')\n",
    "\n",
    "combX_train, combX_test, comb_pipeline = standardize_features(combX_train, combX_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_comb, t_end_comb, comb_params = None, None, None\n",
    "\n",
    "if not use_default:\n",
    "    # number of trees in random forest\n",
    "    n_estimators = [x for x in range(200, 2000, 25)]\n",
    "\n",
    "    # learning rate\n",
    "    arr = np.arange(0.01, 1.0, 0.05)\n",
    "    eta = arr.tolist()\n",
    "\n",
    "    # defines the minimum sum of weights of all observations required in a child\n",
    "    min_child_weight = [i for i in range(1, 100, 8)]\n",
    "\n",
    "    # maximum depth of a tree\n",
    "    max_depth = [i for i in range(1, 250, 5)]\n",
    "\n",
    "    # regularization\n",
    "    gamma = [0, 1, 5]\n",
    "\n",
    "    # number of columns used by each tree\n",
    "    colsample_bytree = [0.8, 0.9, 1]\n",
    "\n",
    "    # maximum number of leaf nodes in tree\n",
    "    max_leaf_nodes = [i for i in range(10, 300, 5)]\n",
    "    max_leaf_nodes.append(None)\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {'n_estimators' : n_estimators,\n",
    "                'eta' : eta,\n",
    "                'max_depth': max_depth,\n",
    "                # 'max_features': max_features,\n",
    "                'min_child_weight': min_child_weight,\n",
    "                'gamma' : gamma,\n",
    "                'colsample_bytree' : colsample_bytree,\n",
    "                'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "    # create xgb model\n",
    "    regressor_rnd = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "    # ensure prediction is made on subsequent data\n",
    "    cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    # random search of parameters, using 3 fold cross validation, \n",
    "    # search across 75 different combinations, and use all available cores\n",
    "    xbg_random = RandomizedSearchCV(estimator = regressor_rnd, param_distributions = random_grid, n_iter = 75, cv = cv,\n",
    "                                verbose=2, random_state=42, n_jobs = -1, error_score='raise')\n",
    "    # fit the xgboost model\n",
    "    t_start_comb = time.time()\n",
    "    xbg_random.fit(combX_train, comby_train)\n",
    "    t_end_comb = time.time()\n",
    "\n",
    "    comb_params = xbg_random.best_params_\n",
    "\n",
    "    print(\"Grid-search took {} seconds.\".format(t_end_comb - t_start_comb))\n",
    "    print(\"Best parameters are:\")\n",
    "    comb_params\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "else:\n",
    "    comb_params = {\n",
    "        'n_estimators': 200,\n",
    "        'min_child_weight': 73,\n",
    "        'max_leaf_nodes': 165,\n",
    "        'max_depth': 31,\n",
    "        'gamma': 1,\n",
    "        'eta': 0.060000000000000005,\n",
    "        'colsample_bytree': 0.9}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "regressor_comb = xgb.XGBRegressor(**comb_params, random_state=42)\n",
    "\n",
    "# fit the random search model\n",
    "t_start_comb = time.time()\n",
    "_ = regressor_comb.fit(combX_train, comby_train)\n",
    "t_end_comb = time.time()\n",
    "\n",
    "print(\"Fitting took {} seconds.\".format(t_end_comb - t_start_comb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "t_start_comb = time.time()\n",
    "comb_test['pred_xgb_all_features'] = regressor_comb.predict(combX_test)\n",
    "t_end_comb = time.time()\n",
    "\n",
    "print(\"Prediction ({} entries) took {} seconds.\".format(len(comb_test), t_end_comb - t_start_comb))\n",
    "\n",
    "maes_comb, rmses_comb = {}, {}\n",
    "\n",
    "maes_comb['all'] = mean_absolute_error(y_true=comb_test.occupancy_rate, y_pred=comb_test.pred_xgb_all_features)\n",
    "rmses_comb['all'] = mean_squared_error(y_true=comb_test.occupancy_rate, y_pred=comb_test.pred_xgb_all_features, squared=False)\n",
    "\n",
    "maes_comb['burgdorf'] = mean_absolute_error(y_true=comb_test[comb_test['loc'] == 'burgdorf'].occupancy_rate, y_pred=comb_test[comb_test['loc'] == 'burgdorf'].pred_xgb_all_features)\n",
    "rmses_comb['burgdorf'] = mean_squared_error(y_true=comb_test[comb_test['loc'] == 'burgdorf'].occupancy_rate, y_pred=comb_test[comb_test['loc'] == 'burgdorf'].pred_xgb_all_features, squared=False)\n",
    "\n",
    "maes_comb['rapperswil'] = mean_absolute_error(y_true=comb_test[comb_test['loc'] == 'rapperswil'].occupancy_rate, y_pred=comb_test[comb_test['loc'] == 'rapperswil'].pred_xgb_all_features)\n",
    "rmses_comb['rapperswil'] = mean_squared_error(y_true=comb_test[comb_test['loc'] == 'rapperswil'].occupancy_rate, y_pred=comb_test[comb_test['loc'] == 'rapperswil'].pred_xgb_all_features, squared=False)\n",
    "\n",
    "for val in ['all', 'burgdorf', 'rapperswil']:\n",
    "    print(\"Prediction for {}:\\tMAE = {}\\tRMSE = {}\".format(val.capitalize(), round(maes_comb[val], 2), round(rmses_comb[val], 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in ['all', 'burgdorf', 'rapperswil']:\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "    if loc == 'all':\n",
    "        y_true = comb_test.occupancy_rate\n",
    "        y_pred = comb_test.pred_xgb_all_features\n",
    "    else:\n",
    "        y_true = comb_test[comb_test['loc'] == loc].occupancy_rate\n",
    "        y_pred = comb_test[comb_test['loc'] == loc].pred_xgb_all_features\n",
    "\n",
    "\n",
    "    # plot the true target values for the test set versus the estimated values with the best model\n",
    "    _ = plt.scatter(comb_test.occupancy_rate, \n",
    "        comb_test.pred_xgb_all_features,\n",
    "        alpha=0.1);\n",
    "    _ = plt.plot([0, 100], [0, 100], \"k--\", lw=3);\n",
    "    _ = plt.xlabel(r'Measured parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.ylabel(r'Predicted parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.title('Combined Model (All features): True values vs predicted values ({})'.format(loc.capitalize()));\n",
    "\n",
    "    # provide MAE and RMSE details to the plot\n",
    "    textstr = '\\n'.join((\n",
    "        r'MAE: %.2f' % (maes_comb[loc]),\n",
    "        r'RMSE: %.2f' % (rmses_comb[loc])\n",
    "    ))\n",
    "\n",
    "    # style infobox\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    _ = plt.text(0.03, 0.95, textstr, verticalalignment='top', bbox=props, transform=plt.gca().transAxes)\n",
    "\n",
    "    if save_figs:\n",
    "        # export - pay attention to an appropriate name\n",
    "        filename = 'combined_f_all_' + loc + '.png'\n",
    "        plt.savefig('../05_visualisations_of_eda/' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rolling_predictions'></a>\n",
    "\n",
    "### 2. Rolling Predictions\n",
    "\n",
    "In the subsequent code section the previously generated models (individual models) are used to perform rolling predictions. This is used to reflect the prediction accuracy over larger time horizons. The following cases are considered:\n",
    "\n",
    "- 12 hours\n",
    "- 24 hours\n",
    "- 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = {\n",
    "    '12h': [12, pd.Timedelta('12h')],\n",
    "    '24h': [24, pd.Timedelta('24h')],\n",
    "    '7d': [168, pd.Timedelta('7d')]\n",
    "}\n",
    "\n",
    "rolling_pred = {}\n",
    "\n",
    "if not use_default:\n",
    "\n",
    "    for loc in locations:\n",
    "        rolling_pred[loc] = pd.DataFrame(columns=horizons.keys()).assign(date=None)\n",
    "        for hor in horizons:\n",
    "            preds = [None] * horizons[hor][0]\n",
    "\n",
    "            rolling_start = time.time()\n",
    "            print('Starting rolling prediction for {} over {}.'.format(loc.capitalize(), hor))\n",
    "\n",
    "            for i in range(0, len(dfs_test[loc])):\n",
    "                # if there is no reference entry left, skip the horizon\n",
    "                if (dfs_test[loc].index[i] + horizons[hor][1] > dfs_test[loc].index[-1]):\n",
    "                    break\n",
    "                \n",
    "                for j in range(0, horizons[hor][0]):\n",
    "                    entry, _ = extract_features(dfs_test[loc].iloc[[(i + j)]], label='occupancy_rate')\n",
    "                    if j > 0:\n",
    "                        entry['t-1'].iloc[0] = preds[j-1]\n",
    "\n",
    "                    if j > 1:\n",
    "                        entry['t-2'].iloc[0] = preds[j-2]\n",
    "                        \n",
    "                    if j > 2:\n",
    "                        entry['t-3'].iloc[0] = preds[j-3]\n",
    "                        \n",
    "                    if j > 6:\n",
    "                        entry['t-7'].iloc[0] = preds[j-7]\n",
    "\n",
    "                    preds[j] = regressors[loc].predict(pipelines[loc].transform(entry))[0]\n",
    "\n",
    "                if horizons[hor][0] == 12:\n",
    "                    newline = [None] * (len(horizons) + 1)\n",
    "                    newline[0] = preds[-1]\n",
    "                    newline[-1] = dfs_test[loc].index[i + horizons[hor][0]]\n",
    "                    rolling_pred[loc].loc[len(rolling_pred[loc])] = newline\n",
    "                else:\n",
    "                    rolling_pred[loc].loc[(rolling_pred[loc].date == dfs_test[loc].index[i + horizons[hor][0]]), hor] = preds[-1]\n",
    "\n",
    "            rolling_end = time.time()\n",
    "            rolling_pred[loc].to_csv('../00_data/{}_{}_rolling.csv'.format(loc, hor))\n",
    "\n",
    "            print('\\ttook {} seconds.'.format(rolling_end - rolling_start))\n",
    "\n",
    "        rolling_pred[loc].set_index('date', inplace=True)\n",
    "\n",
    "else:\n",
    "    for loc in locations:\n",
    "        rolling_pred[loc] = pd.read_csv('../00_data/{}_{}_rolling.csv'.format(loc, list(horizons.keys())[-1]), index_col=0)\n",
    "        rolling_pred[loc].set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes_rolling, rmses_rolling = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    maes_rolling[loc], rmses_rolling[loc] = {}, {}\n",
    "    for hor in horizons:\n",
    "        y_pred = rolling_pred[loc][hor][rolling_pred[loc][hor].notna()]\n",
    "        y_true = dfs[loc].occupancy_rate[(dfs[loc].index >= y_pred.index[0]) & (dfs[loc].index <= y_pred.index[-1])]\n",
    "        maes_rolling[loc][hor] = mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "        rmses_rolling[loc][hor] = mean_squared_error(y_true=y_true, y_pred=y_pred, squared=False)\n",
    "\n",
    "        print(\"Prediction for {} for a horizon of {}:\\tMAE = {}\\tRMSE = {}\".format(loc.capitalize(), hor, round(maes_rolling[loc][hor], 2), round(rmses_rolling[loc][hor], 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rnd_forest'></a>\n",
    "\n",
    "### 3. Random forest\n",
    "\n",
    "For hyperparameter optimization, `random grid search` with cross validation is applied to narrow down the range of reasonable values for the given parameters for the models. Then, `full grid search` with cross validation is applied with the value range obtained from the random grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Define random grid and run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {}\n",
    "\n",
    "if not use_default:\n",
    "    # number of trees in random forest\n",
    "    n_estimators = [x for x in range(200, 2000, 25)]\n",
    "\n",
    "    # number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "\n",
    "    # maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    # important note - consider regularization for the next run\n",
    "\n",
    "    # minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "\n",
    "    # minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    # create grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                'max_features': max_features,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "    for loc in locations:\n",
    "        # create xgb model\n",
    "        regressor_rnd = RandomForestRegressor(random_state=42)\n",
    "\n",
    "        # ensure prediction is made on subsequent data\n",
    "        cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "        # random search of parameters, using 3 fold cross validation, \n",
    "        # search across 75 different combinations, and use all available cores\n",
    "        rf_random = RandomizedSearchCV(estimator = regressor_rnd, param_distributions = random_grid,\n",
    "                                        n_iter = 75, cv = cv,\n",
    "                                        verbose=2, random_state=42, n_jobs = -1)\n",
    "        # fit the random forest model\n",
    "        t_start[loc] = time.time()\n",
    "        rf_random.fit(X_train[loc], y_train[loc])\n",
    "        t_end[loc] = time.time()\n",
    "\n",
    "        params_rf[loc] = rf_random.best_params_\n",
    "\n",
    "        print(\"Grid-search for {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n",
    "        print(\"Best parameters are:\")\n",
    "        params_rf[loc]\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "else:\n",
    "    params_rf['burgdorf'] = {\n",
    "        'n_estimators': 1275,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features': 'auto',\n",
    "        'max_depth': 90\n",
    "    }\n",
    "\n",
    "    params_rf['rapperswil'] = {\n",
    "        'n_estimators': 600,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features': 'auto',\n",
    "        'max_depth': 20\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs = {}\n",
    "maes_rf, rmses_rf = {}, {}\n",
    "t_start, t_end = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    rfs[loc] = RandomForestRegressor(**params_rf[loc], random_state=42)\n",
    "\n",
    "    # fit the random search model\n",
    "    t_start[loc] = time.time()\n",
    "    _ = rfs[loc].fit(X_train[loc], y_train[loc])\n",
    "    t_end[loc] = time.time()\n",
    "\n",
    "    X_test[loc]['random_forest'] = rfs[loc].predict(X_test[loc])\n",
    "\n",
    "    print(\"Prediction for {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n",
    "\n",
    "    maes_rf[loc] = mean_absolute_error(y_true=X_test[loc].occupancy_rate, y_pred=X_test[loc].random_forest)\n",
    "    rmses_rf[loc] = mean_squared_error(y_true=X_test[loc].occupancy_rate, y_pred=X_test[loc].random_forest, squared=False)\n",
    "    \n",
    "    print(\"\\tMAE = {}\\tRMSE = {}\".format(round(maes_rf[loc], 2), round(rmses_rf[loc], 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE: ', round(mean_absolute_error(y_true=df_test['occupancy_rate'], y_pred=df_test['random_forest']), 2))\n",
    "print('RMSE: ', round(mean_squared_error(y_true=df_test['occupancy_rate'], y_pred=df_test['random_forest'], squared=False), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
