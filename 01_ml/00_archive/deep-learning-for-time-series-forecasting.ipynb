{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "82b3b6c2dabd0ad00fcebe67ca5626753fb9365c"
   },
   "source": [
    "<h2><center>Deep Learning for Time Series Forecasting</center></h2>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dimitreOliveira/MachineLearning/master/Kaggle/Store%20Item%20Demand%20Forecasting%20Challenge/time-series%20graph.png\" width=\"800\">\n",
    "\n",
    "### The goal of this notebook is to develop and compare different approaches to time-series problems.\n",
    "\n",
    "#### Content:\n",
    "* [Time series visualization with ploty](#Basic-EDA).\n",
    "* [How to transform a time series dataset into a supervised learning problem](#Transform-the-data-into-a-time-series-problem).\n",
    "* [How to develop a Multilayer Perceptron model for a univariate time series forecasting problem](#MLP-for-Time-Series-Forecasting).\n",
    "* [How to develop a Convolutional Neural Network model for a univariate time series forecasting problem](#CNN-for-Time-Series-Forecasting).\n",
    "* [How to develop a Long Short-Term Memory network model for a univariate time series forecasting problem](#LSTM-for-Time-Series-Forecasting).\n",
    "* [How to develop a Hybrid CNN-LSTM model for a univariate time series forecasting problem](#CNN-LSTM-for-Time-Series-Forecasting).\n",
    "\n",
    "#### The content here was inspired by this article at **machinelearningmastery.com**, [How to Get Started with Deep Learning for Time Series Forecasting (7-Day Mini-Course)](https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/)\n",
    "\n",
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.3.1-py2.py3-none-any.whl (23.9 MB)\n",
      "     |████████████████████████████████| 23.9 MB 2.9 MB/s            \n",
      "\u001b[?25hCollecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from plotly) (1.15.0)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.3.1 tenacity-8.0.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/christopherkindl/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (2.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/christopherkindl/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-macosx_10_11_x86_64.whl (199.0 MB)\n",
      "     |████████████████████████████████| 199.0 MB 17 kB/s              \n",
      "\u001b[?25hCollecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "     |████████████████████████████████| 462 kB 10.8 MB/s            \n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "     |████████████████████████████████| 5.8 MB 6.6 MB/s            \n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     |████████████████████████████████| 65 kB 3.9 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     |████████████████████████████████| 57 kB 5.9 MB/s            \n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "     |████████████████████████████████| 15.6 MB 8.8 MB/s            \n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 2.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "     |████████████████████████████████| 2.9 MB 2.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.17.1)\n",
      "Requirement already satisfied: keras~=2.6 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.38.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.12.0)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.30.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.0)\n",
      "Building wheels for collected packages: clang, termcolor, wrapt\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=9df6afb5b74b6ff7d2333b9b7df3b9b261dcd896573edabc9319d6c821dfd20c\n",
      "  Stored in directory: /Users/christopherkindl/Library/Caches/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=684315a536a626e4dcb7a9145b9167424fdba5438dc4993599ddef20adbb7236\n",
      "  Stored in directory: /Users/christopherkindl/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl size=32668 sha256=a837607cf23a855191de18122b5bb9c20b62131c3cce1f014ef11fecec9885b1\n",
      "  Stored in directory: /Users/christopherkindl/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built clang termcolor wrapt\n",
      "Installing collected packages: wheel, numpy, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Uninstalling wheel-0.34.2:\n",
      "      Successfully uninstalled wheel-0.34.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.3\n",
      "    Uninstalling numpy-1.20.3:\n",
      "      Successfully uninstalled numpy-1.20.3\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.5.0\n",
      "    Uninstalling tensorboard-2.5.0:\n",
      "      Successfully uninstalled tensorboard-2.5.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed astunparse-1.6.3 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-pasta-0.2.0 h5py-3.1.0 keras-preprocessing-1.1.2 numpy-1.19.5 opt-einsum-3.3.0 tensorboard-2.7.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0 wheel-0.37.0 wrapt-1.12.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/christopherkindl/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import plotly.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "from numpy.random import seed\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dcc9f061fb8acd9447b60551e33f8f8d83fd9dad"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing column provided to 'parse_dates': 'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e409ebb9ef6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/christopherkindl/working/start-hack-2021/00_data/00_test_data/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/christopherkindl/working/start-hack-2021/00_data/00_test_data/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_parse_dates_presence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_noconvert_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_validate_parse_dates_presence\u001b[0;34m(self, columns)\u001b[0m\n\u001b[1;32m    274\u001b[0m         )\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;34mf\"Missing column provided to 'parse_dates': '{missing_cols}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Missing column provided to 'parse_dates': 'date'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/Users/christopherkindl/working/start-hack-2021/00_data/00_test_data/train.csv', parse_dates=['date'])\n",
    "test = pd.read_csv('/Users/christopherkindl/working/start-hack-2021/00_data/00_test_data/test.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a60085dbb63091a2947a1f19bdbf2519b0370f1a"
   },
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ae977a0cb52dbbb8701bdf9c4d4393737a98d103"
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9bb6d25dceea154555cabb6c45a83b94fb8dccf1"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "827c9c89b0e144fd4cf4bf2e048e50f98cbe3e26"
   },
   "source": [
    "### Time period of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "98d7855038ae46908b4dbdb9328ca500a6b5d552"
   },
   "outputs": [],
   "source": [
    "print('Min date from train set: %s' % train['date'].min().date())\n",
    "print('Max date from train set: %s' % train['date'].max().date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f67cb93f552c3c7bac8cb8431072501348c4fa53"
   },
   "source": [
    "#### Let's find out what's the time gap between the last day from training set from the last day of the test set, this will be out lag (the amount of day that need to be forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7dd5441edc740bc3254694abb748f49b7ec3bbee"
   },
   "outputs": [],
   "source": [
    "lag_size = (test['date'].max().date() - train['date'].max().date()).days\n",
    "print('Max date from train set: %s' % train['date'].max().date())\n",
    "print('Max date from test set: %s' % test['date'].max().date())\n",
    "print('Forecast lag size', lag_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "455b661280d3dff5b73f43df3fcd82e96e251663"
   },
   "source": [
    "### Basic EDA\n",
    "\n",
    "To explore the time series data first we need to aggregate the sales by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a865dab2049d6157f5c8b1bc90cc37c37b559854"
   },
   "outputs": [],
   "source": [
    "daily_sales = train.groupby('date', as_index=False)['sales'].sum()\n",
    "store_daily_sales = train.groupby(['store', 'date'], as_index=False)['sales'].sum()\n",
    "item_daily_sales = train.groupby(['item', 'date'], as_index=False)['sales'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "153d5030e3381de2679042b1cd77a8889d616e47"
   },
   "source": [
    "### Overall daily sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "79a4ccddbc65b201d6f7b4f55b152ef9eed6762f"
   },
   "outputs": [],
   "source": [
    "daily_sales_sc = go.Scatter(x=daily_sales['date'], y=daily_sales['sales'])\n",
    "layout = go.Layout(title='Daily sales', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\n",
    "fig = go.Figure(data=[daily_sales_sc], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "387e71579b0a4b5aca85a465c8e1b1a3a3289915"
   },
   "source": [
    "### Daily sales by store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0e8715f5557eac3cf3bd92eedec484c76ded1f61"
   },
   "outputs": [],
   "source": [
    "store_daily_sales_sc = []\n",
    "for store in store_daily_sales['store'].unique():\n",
    "    current_store_daily_sales = store_daily_sales[(store_daily_sales['store'] == store)]\n",
    "    store_daily_sales_sc.append(go.Scatter(x=current_store_daily_sales['date'], y=current_store_daily_sales['sales'], name=('Store %s' % store)))\n",
    "\n",
    "layout = go.Layout(title='Store daily sales', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\n",
    "fig = go.Figure(data=store_daily_sales_sc, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15c99b3e7c8fdc1076ff4359697107c15d0da41a"
   },
   "source": [
    "### Daily sales by item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9036b41d4368d8ead697adbcafb50be91ca80c5c"
   },
   "outputs": [],
   "source": [
    "item_daily_sales_sc = []\n",
    "for item in item_daily_sales['item'].unique():\n",
    "    current_item_daily_sales = item_daily_sales[(item_daily_sales['item'] == item)]\n",
    "    item_daily_sales_sc.append(go.Scatter(x=current_item_daily_sales['date'], y=current_item_daily_sales['sales'], name=('Item %s' % item)))\n",
    "\n",
    "layout = go.Layout(title='Item daily sales', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\n",
    "fig = go.Figure(data=item_daily_sales_sc, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd74fe1b76da480ca147af0f68b1e07d479020fc"
   },
   "source": [
    "#### Sub-sample train set to get only the last year of data and reduce training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58a105bc712d2e1da58dfa33cb62f1203de9f0a0"
   },
   "outputs": [],
   "source": [
    "train = train[(train['date'] >= '2017-01-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f30936a69f127157190ecfedcacfd3e50bbb1616"
   },
   "source": [
    "### Rearrange dataset so we can apply shift methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f9ffc8f0946c5caf4d99dc878f83f9e29ee73dd"
   },
   "outputs": [],
   "source": [
    "train_gp = train.sort_values('date').groupby(['item', 'store', 'date'], as_index=False)\n",
    "train_gp = train_gp.agg({'sales':['mean']})\n",
    "train_gp.columns = ['item', 'store', 'date', 'sales']\n",
    "train_gp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72c037d182b0cd9b8d228d0042ace64c88e6cbd9"
   },
   "source": [
    "### Transform the data into a time series problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9400d07223d75b50930e37194c2a682ca436bf2"
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, window=1, lag=1, dropnan=True):\n",
    "    cols, names = list(), list()\n",
    "    # Input sequence (t-n, ... t-1)\n",
    "    for i in range(window, 0, -1):\n",
    "        cols.append(data.shift(i))\n",
    "        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "    # Current timestep (t=0)\n",
    "    cols.append(data)\n",
    "    names += [('%s(t)' % (col)) for col in data.columns]\n",
    "    # Target timestep (t=lag)\n",
    "    cols.append(data.shift(-lag))\n",
    "    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "    # Put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # Drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "76407684060139325ff834aa9215360c7f69b5a8"
   },
   "source": [
    "#### We will use the current timestep and the last 29 to forecast 90 days ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cb604bd4fb5aa6a2fd966af6f7ed772a11ebf6e"
   },
   "outputs": [],
   "source": [
    "window = 29\n",
    "lag = lag_size\n",
    "series = series_to_supervised(train_gp.drop('date', axis=1), window=window, lag=lag)\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f147b4d5ed1cde12436d07fb13fff6c074857f2f"
   },
   "source": [
    "#### Drop rows with different item or store values than the shifted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcc650cb3df2f11a58b4288171663d67cab5f4f7"
   },
   "outputs": [],
   "source": [
    "last_item = 'item(t-%d)' % window\n",
    "last_store = 'store(t-%d)' % window\n",
    "series = series[(series['store(t)'] == series[last_store])]\n",
    "series = series[(series['item(t)'] == series[last_item])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07c8a6ac6dbbe309b626d4e2f47a44fae987875c"
   },
   "source": [
    "#### Remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "830f58c2a435ce18271629008fb1cd79c9323ead"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['item', 'store']]\n",
    "for i in range(window, 0, -1):\n",
    "    columns_to_drop += [('%s(t-%d)' % (col, i)) for col in ['item', 'store']]\n",
    "series.drop(columns_to_drop, axis=1, inplace=True)\n",
    "series.drop(['item(t)', 'store(t)'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdb34762d10c594f403df21519abb22601bf4e01"
   },
   "source": [
    "### Train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "389bfd84bc41a330b2f5ec84dd0ca7517eb52e31"
   },
   "outputs": [],
   "source": [
    "# Label\n",
    "labels_col = 'sales(t+%d)' % lag_size\n",
    "labels = series[labels_col]\n",
    "series = series.drop(labels_col, axis=1)\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(series, labels.values, test_size=0.4, random_state=0)\n",
    "print('Train set shape', X_train.shape)\n",
    "print('Validation set shape', X_valid.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c6dbf5e8699ea64e929009e7c47625eab207535"
   },
   "source": [
    "### MLP for Time Series Forecasting\n",
    "\n",
    "* First we will use a Multilayer Perceptron model or MLP model, here our model will have input features equal to the window size.\n",
    "* The thing with MLP models is that the model don't take the input as sequenced data, so for the model, it is just receiving inputs and don't treat them as sequenced data, that may be a problem since the model won't see the data with the sequence patter that it has.\n",
    "* Input shape **[samples, timesteps]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47962d535b4dddea0588a416ed742b23fdc44db0"
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch = 256\n",
    "lr = 0.0003\n",
    "adam = optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da03328a583580969bd2603c1348b939d533092d"
   },
   "outputs": [],
   "source": [
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\n",
    "model_mlp.add(Dense(1))\n",
    "model_mlp.compile(loss='mse', optimizer=adam)\n",
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "c741ddf5b382617173ef09450a836f940ab4f340"
   },
   "outputs": [],
   "source": [
    "mlp_history = model_mlp.fit(X_train.values, Y_train, validation_data=(X_valid.values, Y_valid), epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ef4d50fedcf9516742b45f3e3a7dab2cb0d301b"
   },
   "source": [
    "### CNN for Time Series Forecasting\n",
    "\n",
    "* For the CNN model we will use one convolutional hidden layer followed by a max pooling layer. The filter maps are then flattened before being interpreted by a Dense layer and outputting a prediction.\n",
    "* The convolutional layer should be able to identify patterns between the timesteps.\n",
    "* Input shape **[samples, timesteps, features]**.\n",
    "\n",
    "#### Data preprocess\n",
    "* Reshape from [samples, timesteps] into [samples, timesteps, features].\n",
    "* This same reshaped data will be used on the CNN and the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00b13a9d49412dd9a50c7a213a042c47e05a53c0"
   },
   "outputs": [],
   "source": [
    "X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n",
    "print('Train set shape', X_train_series.shape)\n",
    "print('Validation set shape', X_valid_series.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "846fccfaa8d3761bbfc57b05ab447e9da07367ee"
   },
   "outputs": [],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(50, activation='relu'))\n",
    "model_cnn.add(Dense(1))\n",
    "model_cnn.compile(loss='mse', optimizer=adam)\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "02870e84ad2997b21994732794251b5d5865211a"
   },
   "outputs": [],
   "source": [
    "cnn_history = model_cnn.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5eac02f27336cb62fa2de354ef647caa3fb4934e"
   },
   "source": [
    "### LSTM for Time Series Forecasting\n",
    "\n",
    "* Now the LSTM model actually sees the input data as a sequence, so it's able to learn patterns from sequenced data (assuming it exists) better than the other ones, especially patterns from long sequences.\n",
    "* Input shape **[samples, timesteps, features]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "522965ca3e2490b8243032c6418ce50d5b724872"
   },
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mse', optimizer=adam)\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "48ac362d2a96028dc2bba1477b3d357c4d79668d"
   },
   "outputs": [],
   "source": [
    "lstm_history = model_lstm.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ba1b9fede4966b07059698f25e7f18e58cab232"
   },
   "source": [
    "### CNN-LSTM for Time Series Forecasting\n",
    "* Input shape **[samples, subsequences, timesteps, features]**.\n",
    "\n",
    "#### Model explanation from the [article](https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/)\n",
    "> \"The benefit of this model is that the model can support very long input sequences that can be read as blocks or subsequences by the CNN model, then pieced together by the LSTM model.\"\n",
    ">\n",
    "> \"When using a hybrid CNN-LSTM model, we will further divide each sample into further subsequences. The CNN model will interpret each sub-sequence and the LSTM will piece together the interpretations from the subsequences. As such, we will split each sample into 2 subsequences of 2 times per subsequence.\"\n",
    ">\n",
    "> \"The CNN will be defined to expect 2 timesteps per subsequence with one feature. The entire CNN model is then wrapped in TimeDistributed wrapper layers so that it can be applied to each subsequence in the sample. The results are then interpreted by the LSTM layer before the model outputs a prediction.\"\n",
    "\n",
    "#### Data preprocess\n",
    "* Reshape from [samples, timesteps, features] into [samples, subsequences, timesteps, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b971641bae57f404bf16dcb7fd4e6fcff47f2be7"
   },
   "outputs": [],
   "source": [
    "subsequences = 2\n",
    "timesteps = X_train_series.shape[1]//subsequences\n",
    "X_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, timesteps, 1))\n",
    "X_valid_series_sub = X_valid_series.reshape((X_valid_series.shape[0], subsequences, timesteps, 1))\n",
    "print('Train set shape', X_train_series_sub.shape)\n",
    "print('Validation set shape', X_valid_series_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57b15ea60310bc7503e76d553e2c6a0d68020c90"
   },
   "outputs": [],
   "source": [
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "model_cnn_lstm.add(LSTM(50, activation='relu'))\n",
    "model_cnn_lstm.add(Dense(1))\n",
    "model_cnn_lstm.compile(loss='mse', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "10c53d1c61050407ce9406093a0fb68d67895b27"
   },
   "outputs": [],
   "source": [
    "cnn_lstm_history = model_cnn_lstm.fit(X_train_series_sub, Y_train, validation_data=(X_valid_series_sub, Y_valid), epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "624df546d7a4f95d8f9bd51d065e472568b25eb2"
   },
   "source": [
    "### Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "47d797354baed8299917542f3a541e61e140e822"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True,figsize=(22,12))\n",
    "ax1, ax2 = axes[0]\n",
    "ax3, ax4 = axes[1]\n",
    "\n",
    "ax1.plot(mlp_history.history['loss'], label='Train loss')\n",
    "ax1.plot(mlp_history.history['val_loss'], label='Validation loss')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_title('MLP')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('MSE')\n",
    "\n",
    "ax2.plot(cnn_history.history['loss'], label='Train loss')\n",
    "ax2.plot(cnn_history.history['val_loss'], label='Validation loss')\n",
    "ax2.legend(loc='best')\n",
    "ax2.set_title('CNN')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('MSE')\n",
    "\n",
    "ax3.plot(lstm_history.history['loss'], label='Train loss')\n",
    "ax3.plot(lstm_history.history['val_loss'], label='Validation loss')\n",
    "ax3.legend(loc='best')\n",
    "ax3.set_title('LSTM')\n",
    "ax3.set_xlabel('Epochs')\n",
    "ax3.set_ylabel('MSE')\n",
    "\n",
    "ax4.plot(cnn_lstm_history.history['loss'], label='Train loss')\n",
    "ax4.plot(cnn_lstm_history.history['val_loss'], label='Validation loss')\n",
    "ax4.legend(loc='best')\n",
    "ax4.set_title('CNN-LSTM')\n",
    "ax4.set_xlabel('Epochs')\n",
    "ax4.set_ylabel('MSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13c7dc56802c04c698310f2fd424d2a001a3e995"
   },
   "source": [
    "#### MLP on train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "17866c9d0ae37cae8e0da41b1d93d7d4049ad043"
   },
   "outputs": [],
   "source": [
    "mlp_train_pred = model_mlp.predict(X_train.values)\n",
    "mlp_valid_pred = model_mlp.predict(X_valid.values)\n",
    "print('Train rmse:', np.sqrt(mean_squared_error(Y_train, mlp_train_pred)))\n",
    "print('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, mlp_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "425f734d0e4c46059138987de98936899ec3bdae"
   },
   "source": [
    "#### CNN on train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "eaa3ca7385ee7c8d884f5e1e6abd54c0aea91293"
   },
   "outputs": [],
   "source": [
    "cnn_train_pred = model_cnn.predict(X_train_series)\n",
    "cnn_valid_pred = model_cnn.predict(X_valid_series)\n",
    "print('Train rmse:', np.sqrt(mean_squared_error(Y_train, cnn_train_pred)))\n",
    "print('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2355313b77222db98a69564449942e3f8378336a"
   },
   "source": [
    "#### LSTM on train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "939f86306a1325c949d8c97262147bcce4f88462"
   },
   "outputs": [],
   "source": [
    "lstm_train_pred = model_lstm.predict(X_train_series)\n",
    "lstm_valid_pred = model_cnn.predict(X_valid_series)\n",
    "print('Train rmse:', np.sqrt(mean_squared_error(Y_train, lstm_train_pred)))\n",
    "print('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, lstm_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c48c38e1cf05cd7503126c5af9c460b743872b9d"
   },
   "source": [
    "#### CNN-LSTM on train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "95e86d4884cb1c2769b194769f98fbc7da8987c3"
   },
   "outputs": [],
   "source": [
    "cnn_lstm_train_pred = model_cnn_lstm.predict(X_train_series_sub)\n",
    "cnn_lstm_valid_pred = model_cnn_lstm.predict(X_valid_series_sub)\n",
    "print('Train rmse:', np.sqrt(mean_squared_error(Y_train, cnn_lstm_train_pred)))\n",
    "print('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_lstm_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "adba93182e270328e63c5c0cb55d0d1ccb344660"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Here you could see some approaches to a time series problem, how to develop and the differences between them, this is not meant to have a great performance, so if you want better results, you are more than welcomed to try a few different hyper-parameters, especially the window size and the networks topology, if you do, please let me know the results.\n",
    "\n",
    "I hope you learned a few things here, leave a feedback and if you liked what you saw make sure to check the [article](https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/) that I used as source.\n",
    "\n",
    "If you want to check out how you can use LSTM as autoencoders and create new features that represent a time series take a look at my other kernel [Time-series forecasting with deep learning & LSTM autoencoders](https://www.kaggle.com/dimitreoliveira/time-series-forecasting-with-lstm-autoencoders/data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
