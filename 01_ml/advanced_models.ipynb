{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Models\n",
    "\n",
    "This IPython notebook contains the entire source code for recreating the advanced model (gradient boosted regressor) as discussed in the work _Predictive Models for Parking Space Occupation_. Please note that running the script `feature_engineering` (found in [feature_engineering.ipynb](./feature_engineering.ipynb)) is a prerequisite to run this notebook.\n",
    "\n",
    "This notebook is divided into `XXX` sections:\n",
    "\n",
    "1. [Dependencies](#dependencies)\n",
    "2. [Data Import](#data_import)\n",
    "3. [Data Preprocessing](#data_preprocessing)\n",
    "4. [Model Training](#model_training)\n",
    "5. [Appendix](#appendix)\n",
    "    1. [Single Model](#single_model)\n",
    "    2. [Rolling Predictions](#rolling_predictions)\n",
    "    3. [Random Forest](#rnd_forest)\n",
    "\n",
    "For the purpose of legibility, code cells are separated, where applicable, to reflect segments required only when running the notebook interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dependencies'></a>\n",
    "\n",
    "## 1. Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries might require pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# machine learning libraries\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full output in Notebook, instead of only the last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the locations to be used.\n",
    "locations = ['burgdorf', 'rapperswil']\n",
    "\n",
    "# Set this to False if another grid-search should be preformed.\n",
    "use_default = True\n",
    "\n",
    "# Set this to False if no tex environment is installed.\n",
    "use_tex = False\n",
    "\n",
    "# Set this to True if plots should be saved to disk.\n",
    "save_figs = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_import'></a>\n",
    "\n",
    "## 2. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../00_data\"\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for loc in locations:\n",
    "    dfs[loc] = pd.read_csv(os.path.join(data_path, \"features_{}.csv\".format(loc)), sep=\",\")\n",
    "    dfs[loc]['date'] = pd.to_datetime(dfs[loc]['date'])\n",
    "\n",
    "    #  declare categorical columns\n",
    "    for col in ['hour', 'day_of_week', 'quarter', 'month', 'day_of_year', 'day_of_month',\n",
    "                'week_of_year', 'weather', 'weather_t-1', 'weather_t-2', 'weather_t-3', 'weather_t-7', \n",
    "                'holiday']:\n",
    "        \n",
    "        dfs[loc][col] = dfs[loc][col].astype(object)\n",
    "\n",
    "    # set datetime column as index\n",
    "    dfs[loc].set_index('date', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape of Burgdorf data: (184, 29)\n",
      "Dataset shape of Rapperswil data: (2455, 29)\n"
     ]
    }
   ],
   "source": [
    "for loc in locations:\n",
    "    print('Dataset shape of {} data: {}'.format(loc.capitalize(), dfs[loc].shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_preprocessing'></a>\n",
    "\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "Split datasets into train and test data using 70/30% ratio by considering the order of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify split date manually and use beginning of the month of the specific date\n",
    "# e.g. 2021-08-16 04:00:00 --> 2021-08-01 01:00:00, only for pragmatic reasons for better paper story\n",
    "splits = {}\n",
    "\n",
    "for loc in locations:\n",
    "    splits[loc] = 0.7 * len(dfs[loc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splits a datetime-indexed dataframe according to a specified split date.\n",
    "# @param df         a datetime-indexed dataframe.\n",
    "# @param split_date the split date.\n",
    "#\n",
    "# @returns A copy of the lower part of the dataframe (including split date) and a copy of the upper part.\n",
    "\n",
    "def split(df:pd.DataFrame, split_date:pd.DatetimeIndex):\n",
    "    # split df into train and test set\n",
    "    df_train = df.loc[df.index <= split_date].copy()\n",
    "    df_test = df.loc[df.index > split_date].copy()\n",
    "    \n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train, dfs_test = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    dfs_train[loc], dfs_test[loc] = split(dfs[loc], dfs[loc].index[int(splits[loc])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff date Burgdorf:\t2021-02-09 11:00:00\t(129/55 entries)\n",
      "Cutoff date Rapperswil:\t2020-11-26 10:00:00\t(1719/736 entries)\n"
     ]
    }
   ],
   "source": [
    "for loc in locations:\n",
    "    print('Cutoff date {}:\\t{}\\t({}/{} entries)'.format(loc.capitalize(), dfs[loc].index[int(splits[loc])], len(dfs_train[loc]), len(dfs_test[loc])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracts a feature column from the dataframe.\\n\",\n",
    "# @param df     the dataframe.\n",
    "# @param label  the label of the column to be extracted.\n",
    "#\n",
    "# @returns the original dataframe and (if found) the column indexed by @p label or (if not found) None.\n",
    "def extract_features(df:pd.DataFrame, label:str=None):\n",
    "    X = df[[# 'date_only', (is not as a feature and kept out-commented to have overview of source data)\n",
    "             'hour',\n",
    "             'day_of_week',\n",
    "             'quarter',\n",
    "             'month',\n",
    "             'day_of_year',\n",
    "             'day_of_month',\n",
    "             'week_of_year',\n",
    "             #'temperature',\n",
    "             'temperature_t-1',\n",
    "             'temperature_t-2',\n",
    "             'temperature_t-3',\n",
    "             'temperature_t-7',\n",
    "             'weather',\n",
    "             'weather_t-1',\n",
    "             'weather_t-2',\n",
    "             'weather_t-3',\n",
    "             'weather_t-7',\n",
    "             'holiday',\n",
    "             't-7',\n",
    "             't-3',\n",
    "             't-2',\n",
    "             't-1']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the occupancy rate as a label\n",
    "X_train, X_test = {}, {}\n",
    "y_train, y_test = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    X_train[loc], y_train[loc] = split_data(dfs_train[loc], label='occupancy_rate')\n",
    "    X_test[loc], y_test[loc] = split_data(dfs_test[loc], label='occupancy_rate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature standardization and scaling\n",
    "\n",
    "Features vary in magnitude and units, which is why feature scaling is applied, using `StandardScaler()` for numeric features and `OneHotEncoder()` for categorical features. For example, the input value `day_of_week` should not be used as a continuous value from 1 to 7, since this would associate a higher weight to the later weekdays (5, 6 or 7) than to the earlier ones (1,2 and 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardize features using standard scaling and one-hot encoding.\n",
    "# @p df_train   the training dataframe.\n",
    "# @p df_test    the test dataframe.\n",
    "#\n",
    "# @returns  the standardized training and test datasets, as well as the generated pipeline object.\n",
    "def standardize_features(df_train:pd.DataFrame, df_test:pd.DataFrame):\n",
    "    # split numerical and categorical columns\n",
    "    data_num = df_train.select_dtypes(include=[np.number])\n",
    "    data_cat = df_train.select_dtypes(include=[object])\n",
    "\n",
    "    # check whether no columns got lost during type allocation\n",
    "    len(df_train.columns) == len(data_num.columns) + len(data_cat.columns)\n",
    "\n",
    "    # create data pipeline\n",
    "    num_pipeline = Pipeline([('std_scaler', StandardScaler())])\n",
    "\n",
    "    num_attribs = list(data_num)\n",
    "    cat_attribs = list(data_cat)\n",
    "\n",
    "    full_pipeline = ColumnTransformer([\n",
    "            ('num', num_pipeline, num_attribs),\n",
    "            # don’t take precautions to handle unseen values for OneHotEncoder\n",
    "            ('cat', OneHotEncoder(handle_unknown = 'ignore'), cat_attribs)])\n",
    "\n",
    "    # fit and transform training data set with preprocessing pipeline and\n",
    "    # only transform test feature set with the pipeline fit on training feature set to not\n",
    "    # artificially improve test performance\n",
    "\n",
    "    return full_pipeline.fit_transform(df_train), full_pipeline.transform(df_test), full_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {}\n",
    "\n",
    "for loc in locations:\n",
    "        X_train[loc], X_test[loc], pipelines[loc] = standardize_features(X_train[loc], X_test[loc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_training'></a>\n",
    "\n",
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameter optimization, we apply `random grid search` with cross validation to narrow down the range of reasonable values for the given parameters for the models. Then, `full grid search` with cross validation is applied with the value range obtained from the random grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Define random grid and run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 75 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 16.2min\n",
      "/Users/christopherkindl/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed: 22.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:50:47] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { max_leaf_nodes } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3),\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None, gamma=None,\n",
       "                                          gpu_id=None, importance_type='gain',\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=na...\n",
       "                                        'max_leaf_nodes': [10, 15, 20, 25, 30,\n",
       "                                                           35, 40, 45, 50, 55,\n",
       "                                                           60, 65, 70, 75, 80,\n",
       "                                                           85, 90, 95, 100, 105,\n",
       "                                                           110, 115, 120, 125,\n",
       "                                                           130, 135, 140, 145,\n",
       "                                                           150, 155, ...],\n",
       "                                        'min_child_weight': [1, 9, 17, 25, 33,\n",
       "                                                             41, 49, 57, 65, 73,\n",
       "                                                             81, 89, 97],\n",
       "                                        'n_estimators': [200, 225, 250, 275,\n",
       "                                                         300, 325, 350, 375,\n",
       "                                                         400, 425, 450, 475,\n",
       "                                                         500, 525, 550, 575,\n",
       "                                                         600, 625, 650, 675,\n",
       "                                                         700, 725, 750, 775,\n",
       "                                                         800, 825, 850, 875,\n",
       "                                                         900, 925, ...]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 75 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 33.4min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed: 48.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:40:05] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { max_leaf_nodes } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=3),\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None, gamma=None,\n",
       "                                          gpu_id=None, importance_type='gain',\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=na...\n",
       "                                        'max_leaf_nodes': [10, 15, 20, 25, 30,\n",
       "                                                           35, 40, 45, 50, 55,\n",
       "                                                           60, 65, 70, 75, 80,\n",
       "                                                           85, 90, 95, 100, 105,\n",
       "                                                           110, 115, 120, 125,\n",
       "                                                           130, 135, 140, 145,\n",
       "                                                           150, 155, ...],\n",
       "                                        'min_child_weight': [1, 9, 17, 25, 33,\n",
       "                                                             41, 49, 57, 65, 73,\n",
       "                                                             81, 89, 97],\n",
       "                                        'n_estimators': [200, 225, 250, 275,\n",
       "                                                         300, 325, 350, 375,\n",
       "                                                         400, 425, 450, 475,\n",
       "                                                         500, 525, 550, 575,\n",
       "                                                         600, 625, 650, 675,\n",
       "                                                         700, 725, 750, 775,\n",
       "                                                         800, 825, 850, 875,\n",
       "                                                         900, 925, ...]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_start, t_end, params = {}, {}, {}\n",
    "\n",
    "if not use_default:\n",
    "    # number of trees in random forest\n",
    "    n_estimators = [x for x in range(200, 2000, 25)]\n",
    "\n",
    "    # learning rate\n",
    "    arr = np.arange(0.01, 1.0, 0.05)\n",
    "    eta = arr.tolist()\n",
    "\n",
    "    # defines the minimum sum of weights of all observations required in a child\n",
    "    min_child_weight = [i for i in range(1, 100, 8)]\n",
    "\n",
    "    # maximum depth of a tree\n",
    "    max_depth = [i for i in range(1, 250, 5)]\n",
    "\n",
    "    # regularization\n",
    "    gamma = [0, 0.5, 1, 3, 5]\n",
    "\n",
    "    # number of columns used by each tree\n",
    "    colsample_bytree = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "    # maximum number of leaf nodes in tree\n",
    "    max_leaf_nodes = [i for i in range(10, 300, 5)]\n",
    "    max_leaf_nodes.append(None)\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {'n_estimators' : n_estimators,\n",
    "                   'eta' : eta,\n",
    "                   'max_depth': max_depth,\n",
    "                # 'max_features': max_features,\n",
    "                   'min_child_weight': min_child_weight,\n",
    "                   'gamma' : gamma,\n",
    "                   'colsample_bytree' : colsample_bytree,\n",
    "                   'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "    for loc in locations:\n",
    "        # create xgb model\n",
    "        regressor_rnd = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "        # ensure prediction is made on subsequent data\n",
    "        cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "        # random search of parameters, using 3 fold cross validation, \n",
    "        # search across 75 different combinations, and use all available cores\n",
    "        xbg_random = RandomizedSearchCV(estimator = regressor_rnd, param_distributions = random_grid,\n",
    "                                        n_iter = 75, cv = cv,\n",
    "                                        verbose=2, random_state=42, n_jobs = -1)\n",
    "        # fit the xgboost model\n",
    "        t_start[loc] = time.time()\n",
    "        xbg_random.fit(X_train[loc], y_train[loc])\n",
    "        t_end[loc] = time.time()\n",
    "\n",
    "        params[loc] = xbg_random.best_params_\n",
    "\n",
    "else:\n",
    "    params['burgdorf'] = {\n",
    "        'n_estimators': 725,\n",
    "        'min_child_weight': 1,\n",
    "        'max_leaf_nodes': 40,\n",
    "        'max_depth': 26,\n",
    "        'gamma': 0,\n",
    "        'eta': 0.11,\n",
    "        'colsample_bytree': 1}\n",
    "        \n",
    "    params['rapperswil'] = {\n",
    "        'n_estimators': 425,\n",
    "        'min_child_weight': 49,\n",
    "        'max_leaf_nodes': 175,\n",
    "        'max_depth': 51,\n",
    "        'gamma': 1,\n",
    "        'eta': 0.01,\n",
    "        'colsample_bytree': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid-search Burgdorf took 1420.4009029865265 seconds.\n",
      "Best parameters are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1575,\n",
       " 'min_child_weight': 17,\n",
       " 'max_leaf_nodes': 170,\n",
       " 'max_depth': 161,\n",
       " 'gamma': 3,\n",
       " 'eta': 0.21000000000000002,\n",
       " 'colsample_bytree': 0.8}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Grid-search Rapperswil took 2916.076371908188 seconds.\n",
      "Best parameters are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 475,\n",
       " 'min_child_weight': 25,\n",
       " 'max_leaf_nodes': 80,\n",
       " 'max_depth': 131,\n",
       " 'gamma': 1,\n",
       " 'eta': 0.01,\n",
       " 'colsample_bytree': 0.9}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not use_default:\n",
    "    for loc in locations:\n",
    "        print(\"Grid-search {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n",
    "        print(\"Best parameters are:\")\n",
    "        params[loc]\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use same procedure to run the `full_grid_search` with the obtained values from `random_grid_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide more granular range based on values from random grid search\n",
    "if not use_default:\n",
    "    for loc in locations:\n",
    "\n",
    "        # number of trees in random forest\n",
    "        n_estimators = [x for x in range(max(params[loc]['n_estimators'] - 25, 1), params[loc]['n_estimators'] + 25, 1)]\n",
    "        \n",
    "        # learning rate\n",
    "        eta = np.array([params[loc]['eta']]).tolist()\n",
    "\n",
    "        # defines the minimum sum of weights of all observations required in a child\n",
    "        min_child_weight = [i for i in range(max(params[loc]['min_child_weight'] - 8, 1), params[loc]['min_child_weight'] + 8, 1)]\n",
    "\n",
    "        # maximum depth of a tree\n",
    "        max_depth = [i for i in range(max(params[loc]['max_depth'] - 5, 1), params[loc]['max_depth'] + 5, 1)]\n",
    "        \n",
    "        # regularization\n",
    "        gamma = [0, 1, 5]\n",
    "        \n",
    "        # number of columns used by each tree\n",
    "        colsample_bytree = [0.6, 0.7, 0.8, 0.9, 1]\n",
    "        \n",
    "        # maximum number of leaf nodes in tree\n",
    "        max_leaf_nodes = [i for i in range(max(params[loc]['max_depth'] - 5, 1), params[loc]['max_depth'] + 5, 1)]\n",
    "        max_leaf_nodes.append(None)\n",
    "        \n",
    "        # create random grid\n",
    "        random_grid = {'n_estimators' : n_estimators,\n",
    "            'eta' : eta,\n",
    "            'max_depth': max_depth,\n",
    "            # 'max_features': max_features,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'gamma' : gamma,\n",
    "            'colsample_bytree' : colsample_bytree,\n",
    "            'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "        # create xgb model\n",
    "        regressor_rnd = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "        # ensure prediction is made on subsequent data\n",
    "        cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "        # random search of parameters, using 3 fold cross validation, \n",
    "        # search across 75 different combinations, and use all available cores\n",
    "        xbg_random = RandomizedSearchCV(estimator = regressor_rnd, param_distributions = random_grid,\n",
    "                                        n_iter = 75, cv = cv,\n",
    "                                        verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "        # xbg_random = GridSearchCV(estimator = regressor_rnd, param_distributions = random_grid,\n",
    "        #                                 n_iter = 75, cv = cv,\n",
    "        #                                 verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "        # fit the xgboost model\n",
    "        t_start[loc] = time.time()\n",
    "        xbg_random.fit(X_train[loc], y_train[loc])\n",
    "        t_end[loc] = time.time()\n",
    "\n",
    "        params[loc] = xbg_random.best_params_\n",
    "\n",
    "else:\n",
    "    params['burgdorf'] = {\n",
    "        'n_estimators': 742,\n",
    "        'min_child_weight': 6,\n",
    "        'max_leaf_nodes': 23,\n",
    "        'max_depth': 22,\n",
    "        'gamma': 5,\n",
    "        'eta': 0.11,\n",
    "        'colsample_bytree': 0.9\n",
    "    }\n",
    "        \n",
    "    params['rapperswil'] = {\n",
    "        'n_estimators': 424,\n",
    "        'min_child_weight': 42,\n",
    "        'max_leaf_nodes': 47,\n",
    "        'max_depth': 55,\n",
    "        'gamma': 0,\n",
    "        'eta': 0.01,\n",
    "        'colsample_bytree': 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_default:\n",
    "    for loc in locations:\n",
    "        print(\"Grid-search {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n",
    "        print(\"Best parameters are:\")\n",
    "        params[loc]\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:39:43] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { max_leaf_nodes } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:40:42] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { max_leaf_nodes } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regressors = {}\n",
    "\n",
    "for loc in locations:\n",
    "    regressors[loc] = xgb.XGBRegressor(**params[loc], random_state=42)\n",
    "\n",
    "    # fit the random search model\n",
    "    t_start[loc] = time.time()\n",
    "    _ = regressors[loc].fit(X_train[loc], y_train[loc])\n",
    "    t_end[loc] = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print(\"Fitting for {} took {} seconds.\".format(loc.capitalize(), t_end[loc] - t_start[loc]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes, rmses = {}, {}\n",
    "\n",
    "for loc in locations:\n",
    "    t_start[loc] = time.time()\n",
    "    dfs_test[loc]['pred_xgb_all_features'] = regressors[loc].predict(X_test[loc])\n",
    "    t_end[loc] = time.time()\n",
    "    maes[loc] = mean_absolute_error(y_true=dfs_test[loc]['occupancy_rate'], y_pred=dfs_test[loc]['pred_xgb_all_features'])\n",
    "    rmses[loc] = mean_squared_error(y_true=dfs_test[loc]['occupancy_rate'], y_pred=dfs_test[loc]['pred_xgb_all_features'], squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Burgdorf (2332 entries) took 0.1355271339416504 seconds.\n",
      "\tMAE:  2.48\n",
      "\tRMSE:  3.58\n",
      "Prediction for Rapperswil (3692 entries) took 0.20877504348754883 seconds.\n",
      "\tMAE:  4.09\n",
      "\tRMSE:  6.12\n"
     ]
    }
   ],
   "source": [
    "for loc in locations:\n",
    "    print(\"Prediction for {} ({} entries) took {} seconds.\".format(loc.capitalize(), len(dfs_test[loc]), t_end[loc] - t_start[loc]))\n",
    "    print(\"\\tMAE: \", round(maes[loc], 2))\n",
    "    print(\"\\tRMSE: \", round(rmses[loc], 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Burgdorf (2332 entries) took 0.24286103248596191 seconds.\n",
      "\tMAE:  2.5\n",
      "\tRMSE:  3.62\n",
      "Prediction for Rapperswil (3692 entries) took 0.20580506324768066 seconds.\n",
      "\tMAE:  4.08\n",
      "\tRMSE:  6.12\n"
     ]
    }
   ],
   "source": [
    "for loc in locations:\n",
    "    print(\"Prediction for {} ({} entries) took {} seconds.\".format(loc.capitalize(), len(dfs_test[loc]), t_end[loc] - t_start[loc]))\n",
    "    print(\"\\tMAE: \", round(maes[loc], 2))\n",
    "    print(\"\\tRMSE: \", round(rmses[loc], 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_params = {\n",
    "    'text.usetex' : use_tex,\n",
    "    'font.size' : 20,\n",
    "    'xtick.labelsize' : 18,\n",
    "    'ytick.labelsize' : 18,\n",
    "    'lines.linewidth': 1,\n",
    "    'grid.linewidth':   2,\n",
    "}\n",
    "plt.rcParams.update(plt_params)\n",
    "\n",
    "for loc in ['burgdorf', 'rapperswil']:\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "    # plot the true target values for the test set versus the estimated values with the best model\n",
    "    _ = plt.scatter(dfs_test[loc]['occupancy_rate'], \n",
    "        dfs_test[loc]['pred_xgb_all_features'],\n",
    "        alpha=0.1);\n",
    "    _ = plt.plot([0, 100], [0, 100], \"k--\", lw=3);\n",
    "    _ = plt.xlabel(r'Measured parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.ylabel(r'Predicted parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.title('All features: True values vs predicted values ({})'.format(loc.capitalize()));\n",
    "\n",
    "    # provide MAE and RMSE details to the plot\n",
    "    textstr = '\\n'.join((\n",
    "        r'MAE: %.2f' % (maes[loc]),\n",
    "        r'RMSE: %.2f' % (rmses[loc])\n",
    "    ))\n",
    "\n",
    "    # style infobox\n",
    "    # plt.gca().hist(x, 20)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    _ = plt.text(0.03, 0.95, textstr, verticalalignment='top', bbox=props, transform=plt.gca().transAxes)\n",
    "\n",
    "    if save_figs:\n",
    "        # export - pay attention to an appropriate name\n",
    "        filename = 'f_all_' + loc + '.png'\n",
    "        plt.savefig('../05_visualisations_of_eda/' + filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='appendix'></a>\n",
    "## 5. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single_model'></a>\n",
    "\n",
    "### 1. Single Model\n",
    "\n",
    "In the subsequent code section a single model is generated in order to predict occupancies for multiple parking locations. The data is selected such that only entries from a date range present in both datasets are considered. In order to destinguish the data, an additional column `loc` is added, which is a categorical feature and thus one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find overlapping window\n",
    "start_date = max(dfs['burgdorf'].index[0], dfs['rapperswil'].index[0])\n",
    "end_date = min(dfs['burgdorf'].index[-1], dfs['rapperswil'].index[-1])\n",
    "\n",
    "if not (start_date < end_date):\n",
    "    raise AssertionError(\"Non-overlapping time windows!\")\n",
    "\n",
    "# Merge dataframes by time and location name\n",
    "dfcomb = dfs['burgdorf'].loc[(dfs['burgdorf'].index >= start_date) & (dfs['burgdorf'].index <= end_date)].assign(loc='burgdorf')\n",
    "dfcomb = dfcomb.append(dfs['rapperswil'].loc[(dfs['rapperswil'].index >= start_date) & (dfs['rapperswil'].index <= end_date)].assign(loc='rapperswil'))\n",
    "dfcomb.sort_values(['date', 'loc'],inplace=True)\n",
    "\n",
    "print('Merged datasets (total {} entries)'.format(len(dfcomb)))\n",
    "\n",
    "# Perform Train/Test split\n",
    "splitcomb = 0.7 * len(dfcomb)\n",
    "comb_train, comb_test = split(dfcomb, dfcomb.index[int(splitcomb)])\n",
    "print('Cutoff date:\\t{}\\t({}/{} entries)'.format(dfcomb.index[int(splitcomb)], len(comb_train), len(comb_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracts a feature column from the combined dataframe.\n",
    "# @param df     the dataframe.\n",
    "# @param label  the label of the column to be extracted.\n",
    "#\n",
    "# @returns the original dataframe and (if found) the column indexed by @p label or (if not found) None.\n",
    "def extract_combined_features(df:pd.DataFrame, label:str=None):\n",
    "    X = df[[# 'date_only', (is not as a feature and kept out-commented to have overview of source data)\n",
    "             'hour',\n",
    "             'day_of_week',\n",
    "             'quarter',\n",
    "             'month',\n",
    "             'day_of_year',\n",
    "             'day_of_month',\n",
    "             'week_of_year',\n",
    "             'temperature',\n",
    "             'weather',\n",
    "             'holiday',\n",
    "             't-7',\n",
    "             't-3',\n",
    "             't-2',\n",
    "             't-1',\n",
    "             'loc']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combX_train, comby_train = extract_combined_features(comb_train, label='occupancy_rate')\n",
    "combX_test, comby_test = extract_combined_features(comb_test, label='occupancy_rate')\n",
    "\n",
    "combX_train, combX_test, comb_pipeline = standardize_features(combX_train, combX_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_comb, t_end_comb, comb_params = None, None, None\n",
    "\n",
    "if not use_default:\n",
    "    # number of trees in random forest\n",
    "    n_estimators = [x for x in range(200, 2000, 25)]\n",
    "\n",
    "    # learning rate\n",
    "    arr = np.arange(0.01, 1.0, 0.05)\n",
    "    eta = arr.tolist()\n",
    "\n",
    "    # defines the minimum sum of weights of all observations required in a child\n",
    "    min_child_weight = [i for i in range(1, 100, 8)]\n",
    "\n",
    "    # maximum depth of a tree\n",
    "    max_depth = [i for i in range(1, 250, 5)]\n",
    "\n",
    "    # regularization\n",
    "    gamma = [0, 1, 5]\n",
    "\n",
    "    # number of columns used by each tree\n",
    "    colsample_bytree = [0.8, 0.9, 1]\n",
    "\n",
    "    # maximum number of leaf nodes in tree\n",
    "    max_leaf_nodes = [i for i in range(10, 300, 5)]\n",
    "    max_leaf_nodes.append(None)\n",
    "\n",
    "    # create random grid\n",
    "    random_grid = {'n_estimators' : n_estimators,\n",
    "                'eta' : eta,\n",
    "                'max_depth': max_depth,\n",
    "                # 'max_features': max_features,\n",
    "                'min_child_weight': min_child_weight,\n",
    "                'gamma' : gamma,\n",
    "                'colsample_bytree' : colsample_bytree,\n",
    "                'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "    # create xgb model\n",
    "    regressor_rnd = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "    # ensure prediction is made on subsequent data\n",
    "    cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    # random search of parameters, using 3 fold cross validation, \n",
    "    # search across 75 different combinations, and use all available cores\n",
    "    xbg_random = RandomizedSearchCV(estimator = regressor_rnd, param_distributions = random_grid, n_iter = 75, cv = cv,\n",
    "                                verbose=2, random_state=42, n_jobs = -1, error_score='raise')\n",
    "    # fit the xgboost model\n",
    "    t_start_comb = time.time()\n",
    "    xbg_random.fit(combX_train, comby_train)\n",
    "    t_end_comb = time.time()\n",
    "\n",
    "    comb_params = xbg_random.best_params_\n",
    "\n",
    "    print(\"Grid-search took {} seconds.\".format(t_end_comb - t_start_comb))\n",
    "    print(\"Best parameters are:\")\n",
    "    comb_params\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "else:\n",
    "    comb_params = {\n",
    "        'n_estimators': 200,\n",
    "        'min_child_weight': 73,\n",
    "        'max_leaf_nodes': 165,\n",
    "        'max_depth': 31,\n",
    "        'gamma': 1,\n",
    "        'eta': 0.060000000000000005,\n",
    "        'colsample_bytree': 0.9}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "regressor_comb = xgb.XGBRegressor(**comb_params, random_state=42)\n",
    "\n",
    "# fit the random search model\n",
    "t_start_comb = time.time()\n",
    "_ = regressor_comb.fit(combX_train, comby_train)\n",
    "t_end_comb = time.time()\n",
    "\n",
    "print(\"Fitting took {} seconds.\".format(t_end_comb - t_start_comb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "t_start_comb = time.time()\n",
    "comb_test['pred_xgb_all_features'] = regressor_comb.predict(combX_test)\n",
    "t_end_comb = time.time()\n",
    "\n",
    "print(\"Prediction ({} entries) took {} seconds.\".format(len(comb_test), t_end_comb - t_start_comb))\n",
    "\n",
    "maes_comb, rmses_comb = {}, {}\n",
    "\n",
    "maes_comb['all'] = mean_absolute_error(y_true=comb_test.occupancy_rate, y_pred=comb_test.pred_xgb_all_features)\n",
    "rmses_comb['all'] = mean_squared_error(y_true=comb_test.occupancy_rate, y_pred=comb_test.pred_xgb_all_features, squared=False)\n",
    "\n",
    "maes_comb['burgdorf'] = mean_absolute_error(y_true=comb_test[comb_test['loc'] == 'burgdorf'].occupancy_rate, y_pred=comb_test[comb_test['loc'] == 'burgdorf'].pred_xgb_all_features)\n",
    "rmses_comb['burgdorf'] = mean_squared_error(y_true=comb_test[comb_test['loc'] == 'burgdorf'].occupancy_rate, y_pred=comb_test[comb_test['loc'] == 'burgdorf'].pred_xgb_all_features, squared=False)\n",
    "\n",
    "maes_comb['rapperswil'] = mean_absolute_error(y_true=comb_test[comb_test['loc'] == 'rapperswil'].occupancy_rate, y_pred=comb_test[comb_test['loc'] == 'rapperswil'].pred_xgb_all_features)\n",
    "rmses_comb['rapperswil'] = mean_squared_error(y_true=comb_test[comb_test['loc'] == 'rapperswil'].occupancy_rate, y_pred=comb_test[comb_test['loc'] == 'rapperswil'].pred_xgb_all_features, squared=False)\n",
    "\n",
    "for val in ['all', 'burgdorf', 'rapperswil']:\n",
    "    print(\"Prediction for {}:\\tMAE = {}\\tRMSE = {}\".format(val.capitalize(), round(maes_comb[val], 2), round(rmses_comb[val], 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in ['all', 'burgdorf', 'rapperswil']:\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "    if loc == 'all':\n",
    "        y_true = comb_test.occupancy_rate\n",
    "        y_pred = comb_test.pred_xgb_all_features\n",
    "    else:\n",
    "        y_true = comb_test[comb_test['loc'] == loc].occupancy_rate\n",
    "        y_pred = comb_test[comb_test['loc'] == loc].pred_xgb_all_features\n",
    "\n",
    "\n",
    "    # plot the true target values for the test set versus the estimated values with the best model\n",
    "    _ = plt.scatter(comb_test.occupancy_rate, \n",
    "        comb_test.pred_xgb_all_features,\n",
    "        alpha=0.1);\n",
    "    _ = plt.plot([0, 100], [0, 100], \"k--\", lw=3);\n",
    "    _ = plt.xlabel(r'Measured parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.ylabel(r'Predicted parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.title('Combined Model (All features): True values vs predicted values ({})'.format(loc.capitalize()));\n",
    "\n",
    "    # provide MAE and RMSE details to the plot\n",
    "    textstr = '\\n'.join((\n",
    "        r'MAE: %.2f' % (maes_comb[loc]),\n",
    "        r'RMSE: %.2f' % (rmses_comb[loc])\n",
    "    ))\n",
    "\n",
    "    # style infobox\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    _ = plt.text(0.03, 0.95, textstr, verticalalignment='top', bbox=props, transform=plt.gca().transAxes)\n",
    "\n",
    "    if save_figs:\n",
    "        # export - pay attention to an appropriate name\n",
    "        filename = 'combined_f_all_' + loc + '.png'\n",
    "        plt.savefig('../05_visualisations_of_eda/' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rolling_predictions'></a>\n",
    "\n",
    "### 2. Rolling Predictions\n",
    "\n",
    "In the subsequent code section the previously generated models (individual models) are used to perform rolling predictions. This is used to reflect the prediction accuracy over larger time horizons. The following cases are considered:\n",
    "\n",
    "- 12 hours\n",
    "- 24 hours\n",
    "- 7 days\n",
    "- 14 days\n",
    "- 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = {\n",
    "    '12h': [12, pd.Timedelta('12h')],\n",
    "    '24h': [24, pd.Timedelta('24h')],\n",
    "    '7d': [168, pd.Timedelta('7d')],\n",
    "    # '14d': [336, pd.Timedelta('14d')],\n",
    "    # '30d': [720, pd.Timedelta('30d')]\n",
    "}\n",
    "\n",
    "rolling_pred = {}\n",
    "\n",
    "for loc in locations:\n",
    "    for hor in horizons:\n",
    "        rolling_pred[loc] = pd.DataFrame(columns=horizons.keys()).assign(date=None)\n",
    "        preds = [None] * horizons[hor][0]\n",
    "        for i in range(0, len(dfs_test[loc])):\n",
    "            # if there is no reference entry left, skip the horizon\n",
    "            if (dfs_test[loc].index[i] + horizons[hor][1] > dfs_test[loc].index[-1]):\n",
    "                break\n",
    "            \n",
    "            for j in range(0, horizons[hor][0]):\n",
    "                entry, _ = extract_features(dfs_test[loc].iloc[[(i + j)]], label='occupancy_rate')\n",
    "                if j > 0:\n",
    "                    entry['t-1'].iloc[0] = preds[j-1]\n",
    "\n",
    "                if j > 1:\n",
    "                    entry['t-2'].iloc[0] = preds[j-2]\n",
    "                    \n",
    "                if j > 2:\n",
    "                    entry['t-3'].iloc[0] = preds[j-3]\n",
    "                    \n",
    "                if j > 6:\n",
    "                    entry['t-7'].iloc[0] = preds[j-7]\n",
    "\n",
    "                preds[j] = regressors[loc].predict(pipelines[loc].transform(entry))[0]\n",
    "\n",
    "            if horizons[hor][0] == 12:\n",
    "                newline = [None] * (len(horizons) + 1)\n",
    "                newline[0] = preds[-1]\n",
    "                newline[-1] = dfs_test[loc].index[i + horizons[hor][0]]\n",
    "                rolling_pred[loc].loc[len(rolling_pred[loc])] = newline\n",
    "            else:\n",
    "                rolling_pred[loc].loc[(rolling_pred[loc].date == dfs_test[loc].index[i + horizons[hor][0]]), hor] = preds[-1]\n",
    "\n",
    "        rolling_pred[loc].to_pickle('../00_data/{}_{}_rolling.pckl'.format(loc, hor))\n",
    "\n",
    "    rolling_pred[loc].set_index('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print(rolling_pred[loc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Stuff todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# customized preprocessing functions\n",
    "import util\n",
    "\n",
    "# standard libraries\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# models\n",
    "from pmdarima import auto_arima\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_preds = {}\n",
    "\n",
    "for loc in locations:\n",
    "    new_preds[loc] = [None] * len(dfs_test[loc])\n",
    "    for i in range(0, len(dfs_test[loc])):\n",
    "        entry, _ = extract_features(dfs_test[loc].iloc[[i]], label='occupancy_rate')\n",
    "\n",
    "        if i > 0:\n",
    "            entry['t-1'].iloc[0] = new_preds[loc][i-1]\n",
    "\n",
    "        if i > 1:\n",
    "            entry['t-2'].iloc[0] = new_preds[loc][i-2]\n",
    "            \n",
    "        if i > 2:\n",
    "            entry['t-3'].iloc[0] = new_preds[loc][i-3]\n",
    "            \n",
    "        if i > 6:\n",
    "            entry['t-7'].iloc[0] = new_preds[loc][i-7]\n",
    "\n",
    "        new_preds[loc][i] = regressors[loc].predict(pipelines[loc].transform(entry))\n",
    "        \n",
    "# X, y = extract_features(dfs_test['burgdorf'].iloc[[0]], label='occupancy_rate')\n",
    "\n",
    "# X = pipelines['burgdorf'].transform(X)\n",
    "\n",
    "# print(dfs_test['burgdorf'].iloc[0])\n",
    "# print(regressors['burgdorf'].predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    print(\"Prediction for {} ({} entries) took {} seconds.\".format(loc.capitalize(), len(dfs_test[loc]), t_end[loc] - t_start[loc]))\n",
    "    print(\"\\tMAE: \", round(mean_absolute_error(y_true=dfs_test[loc]['occupancy_rate'], y_pred=new_preds[loc]), 2))\n",
    "    print(\"\\tRMSE: \", round(mean_squared_error(y_true=dfs_test[loc]['occupancy_rate'], y_pred=new_preds[loc], squared=False), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rnd_forest'></a>\n",
    "\n",
    "### 1. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameter optimization, `random grid search` with cross validation is applied to narrow down the range of reasonable values for the given parameters for the models. Then, `full grid search` with cross validation is applied with the value range obtained from the random grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Define random grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trees in random forest\n",
    "n_estimators = [x for x in range(200, 2000, 25)]\n",
    "\n",
    "# number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# important note - consider regularization for the next run\n",
    "\n",
    "# minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# create grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the random grid to search for best hyperparameters\n",
    "\n",
    "# create random forest model\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "# ensure prediction is made on subsequent data\n",
    "cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# random search of parameters, using 3 fold cross validation, \n",
    "# search across 75 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 75, cv = cv,\n",
    "                               verbose=2, random_state=42, n_jobs = -1)\n",
    "# fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "#s how best parameters\n",
    "print('Best parameters:')\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Define new grid with optimal value range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define more specific grid\n",
    "\n",
    "# number of trees in random forest\n",
    "n_estimators = [x for x in range(200, 2000, 25)]\n",
    "\n",
    "# number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# create grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random forest model\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "# ensure prediction is made on subsequent data\n",
    "cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# random search of parameters, using 3 fold cross validation, \n",
    "# search across 75 different combinations, and use all available cores\n",
    "rf = GridSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 75, cv = cv,\n",
    "                               verbose=2, random_state=42, n_jobs = -1)\n",
    "# fit the random search model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# show best parameters\n",
    "print('Best parameters:')\n",
    "rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Fit model with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state = 42,\n",
    "                           n_estimators = 600,\n",
    "                           max_features = 'auto',\n",
    "                           max_depth = 20,\n",
    "                           min_samples_split = 2,\n",
    "                           min_samples_leaf = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the random search model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# run prediction\n",
    "df_test['random_forest'] = rf.predict(X_test)\n",
    "df_all = pd.concat([df_test, df_train], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE: ', round(mean_absolute_error(y_true=df_test['occupancy_rate'], y_pred=df_test['random_forest']), 2))\n",
    "print('RMSE: ', round(mean_squared_error(y_true=df_test['occupancy_rate'], y_pred=df_test['random_forest'], squared=False), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xgboost'></a>\n",
    "\n",
    "### 2. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Performance: 3.93 (MAE), 6.03 (RMSE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state = 42,\n",
    "                           n_estimators = 600,\n",
    "                           max_features = 'auto',\n",
    "                           max_depth = 20,\n",
    "                           min_samples_split = 2,\n",
    "                           min_samples_leaf = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'text.usetex' : use_tex,\n",
    "    'font.size' : 20,\n",
    "    'xtick.labelsize' : 18,\n",
    "    'ytick.labelsize' : 18,\n",
    "    'lines.linewidth': 1,\n",
    "    'grid.linewidth':   2,\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "for loc in ['burgdorf', 'rapperswil']:\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "    # plot the true target values for the test set versus the estimated values with the best model\n",
    "    _ = plt.scatter(dfs_test[loc]['occupancy_rate'], \n",
    "        dfs_test[loc]['pred_xgb_all_features'],\n",
    "        alpha=0.1);\n",
    "    _ = plt.plot([0, 100], [0, 100], \"k--\", lw=3);\n",
    "    _ = plt.xlabel(r'Measured parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.ylabel(r'Predicted parking occupancy $\\left[\\%\\right]$');\n",
    "    _ = plt.title('All features: True values vs predicted values ({})'.format(loc.capitalize()));\n",
    "\n",
    "    # provide MAE and RMSE details to the plot\n",
    "    textstr = '\\n'.join((\n",
    "        r'MAE: %.2f' % (mean_absolute_error(y_true=dfs_test[loc]['occupancy_rate'], y_pred=dfs_test[loc]['pred_xgb_all_features'])),\n",
    "        r'RMSE: %.2f' % (mean_squared_error(y_true=dfs_test[loc]['occupancy_rate'], y_pred=dfs_test[loc]['pred_xgb_all_features'], squared=False))\n",
    "    ))\n",
    "\n",
    "    # style infobox\n",
    "    # plt.gca().hist(x, 20)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    _ = plt.text(0.03, 0.95, textstr, verticalalignment='top', bbox=props, transform=plt.gca().transAxes)\n",
    "\n",
    "# export - pay attention to an appropriate name\n",
    "# file = 'f1_features.png'\n",
    "# plt.savefig('../05_visualisations_of_eda/' + file);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
