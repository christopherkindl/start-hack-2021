{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "from airflow.models import Variable\n",
    "from airflow.hooks.S3_hook import S3Hook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Set up the main configurations of the dag\n",
    "# =============================================================================\n",
    "# now = datetime.now() # current date and time\n",
    "# date_time = now.strftime(\"%Y_%m_%d_%HH\")\n",
    "# print(\"date and time:\",date_time)\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2021, 3, 8),\n",
    "    'owner': 'Airflow',\n",
    "    'filestore_base': '/tmp/airflowtemp/',\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'aws_conn_id': \"aws_default_carinatiedemann\",\n",
    "    'bucket_name': Variable.get(\"housing_web_scraping_pipeline\", deserialize_json=True)['bucket_name'],\n",
    "    'postgres_conn_id': 'engineering_groupwork_carina',\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'db_name': Variable.get(\"housing_db\", deserialize_json=True)['db_name']\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG('housing_web_scraping_pipeline',\n",
    "          description='Web scraping pipeline scraping housing data and saving output to a postgreQL db in RDS',\n",
    "          schedule_interval='@hourly',\n",
    "          catchup=False,\n",
    "          default_args=default_args,\n",
    "          max_active_runs=1)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Define different functions\n",
    "# =============================================================================\n",
    "\n",
    "# Creating schema if inexistant\n",
    "def create_schema(**kwargs):\n",
    "    pg_hook = PostgresHook(postgres_conn_id=kwargs['postgres_conn_id'], schema=kwargs['db_name'])\n",
    "    conn = pg_hook.get_conn()\n",
    "    cursor = conn.cursor()\n",
    "    log.info('Initialised connection')\n",
    "    sql_queries = \"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS schema_housing;\n",
    "    DROP TABLE IF EXISTS schema_housing.zoopla;\n",
    "    CREATE TABLE IF NOT EXISTS schema_housing.zoopla(\n",
    "        \"ad_id\" numeric,\n",
    "        \"link\" varchar(256),\n",
    "        \"price\" numeric,\n",
    "        \"bedrooms\" numeric,\n",
    "        \"bathrooms\" numeric,\n",
    "        \"living_rooms\" numeric,\n",
    "        \"address\" varchar(256),\n",
    "        \"distance\" numeric,\n",
    "        \"subway_station\" varchar(256)\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(sql_queries)\n",
    "    conn.commit()\n",
    "    log.info(\"Created Schema and Tables\")\n",
    "\n",
    "# Webscraping Zoopla\n",
    "def web_scraping_function_zoopla(**kwargs):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    #import matplotlib.pyplot as plt\n",
    "    import time\n",
    "    from bs4 import BeautifulSoup #requires pip install\n",
    "    import requests\n",
    "    import re\n",
    "    from re import sub\n",
    "    from decimal import Decimal\n",
    "    import io\n",
    "\n",
    "    # Convert price string into a numerical value\n",
    "    def to_num(price):\n",
    "        value = Decimal(sub(r'[^\\d.]', '', price))\n",
    "        return float(value)\n",
    "\n",
    "    def is_dropped(money):\n",
    "        for i in range(len(money)):\n",
    "            if(money[i] != 'Â£' and money[i] != ',' and (not money[i].isdigit())):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    #set up the the scraper\n",
    "    url = 'https://www.zoopla.co.uk/for-sale/property/london/?page_size=25&q=london&radius=0&results_sort=newest_listings&pn='\n",
    "\n",
    "    map = {}\n",
    "    id = 0\n",
    "\n",
    "    #set max_pages to 2 for test purposes\n",
    "    max_pages = 300\n",
    "\n",
    "    # time.sleep(10)\n",
    "    start = time.time()\n",
    "\n",
    "    for p in range(max_pages):\n",
    "        cur_url = url + str(p + 1)\n",
    "\n",
    "        print(\"Scraping page: %d\" % (p + 1))\n",
    "        #print(cur_url)\n",
    "        html_text = requests.get(cur_url).text\n",
    "        soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "        ads = soup.find_all('div', class_ = 'css-wfndrn-StyledContent e2uk8e18')\n",
    "        page_nav = soup.find_all('a', class_ = 'css-slm4qd-StyledPaginationLink eaoxhri5')\n",
    "\n",
    "        if(len(page_nav) == 0):\n",
    "            print(\"max page number: %d\" % (p))\n",
    "            # end = time.time()\n",
    "            # print(end - start)\n",
    "            break\n",
    "\n",
    "        for k in range(len(ads)):\n",
    "            ad = ads[k]\n",
    "\n",
    "            #find link and ID ('identifier' in the link acts as a unique id for the ad)\n",
    "            link = ad.find('a', class_ = 'e2uk8e4 css-gl9725-StyledLink-Link-FullCardLink e33dvwd0')\n",
    "\n",
    "            #find section for address\n",
    "            address = ad.find('p', class_ = 'css-wfe1rf-Text eczcs4p0').text\n",
    "\n",
    "            #find price information\n",
    "            price = ad.find('p', class_ = 'css-18tfumg-Text eczcs4p0').text\n",
    "\n",
    "            # if the price is valid or not, if not we do not consider this ad\n",
    "            if(is_dropped(price)): continue\n",
    "\n",
    "            #find public transport information\n",
    "            subway_section = ad.find('div', class_ = 'css-braguw-TransportWrapper e2uk8e28')\n",
    "            subway_information = subway_section.find_all('p', class_ = 'css-wfe1rf-Text eczcs4p0')\n",
    "\n",
    "            #skip ads that only contain information of train station\n",
    "            outlier = subway_section.find('span', class_ = 'e1uy4ban0 css-10ibqwe-StyledIcon-Icon e15462ye0')\n",
    "            if(outlier['data-testid'] == 'national_rail_station'): continue\n",
    "\n",
    "            #find section for bedroom, bathroom and living room information (room numbers)\n",
    "            feature_section = ad.find('div', class_ = 'css-58bgfg-WrapperFeatures e2uk8e15')\n",
    "\n",
    "            #find all information available for room numbers\n",
    "            category = feature_section.find_all('div', class_ = 'ejjz7ko0 css-l6ka86-Wrapper-IconAndText e3e3fzo1')\n",
    "\n",
    "            #assign id\n",
    "            ad_id = link['href'] #returns url snippet with identifier from the url\n",
    "            ad_id= ad_id.split(\"?\")[0] #split by '?' ans '/' and apply index to retain only identifier number\n",
    "            ad_id= ad_id.split(\"/\")[3]\n",
    "\n",
    "            if(ad_id in map): continue\n",
    "            map[ad_id] = {}\n",
    "\n",
    "            #assign link\n",
    "            link = 'https://www.zoopla.co.uk/' + link['href']\n",
    "            map[ad_id][\"link\"] = link\n",
    "\n",
    "            #assign address\n",
    "            map[ad_id][\"address\"] = address\n",
    "\n",
    "            #assign bedroom nr\n",
    "            try:\n",
    "                map[ad_id][\"room_nr\"] = category[0].text\n",
    "            except IndexError:\n",
    "            #insert None value if index is not found\n",
    "                map[ad_id][\"room_nr\"] = 'None'\n",
    "                #print(\"Feature not listed\")\n",
    "\n",
    "            #assign bathroom nr\n",
    "            try:\n",
    "                map[ad_id][\"bath_nr\"] = category[1].text\n",
    "            except IndexError:\n",
    "            #insert None value if index is not found\n",
    "                map[ad_id][\"bath_nr\"] = 'None'\n",
    "                #print(\"Feature not listed\")\n",
    "\n",
    "            #assign living room nr\n",
    "            try:\n",
    "                map[ad_id][\"living_nr\"] = category[2].text\n",
    "            except IndexError:\n",
    "            #insert None value if index is not found\n",
    "                map[ad_id][\"living_nr\"] = 'None'\n",
    "                #print(\"Feature not listed\")\n",
    "\n",
    "            #assign price\n",
    "            map[ad_id][\"price\"] = to_num(price)\n",
    "\n",
    "            #assign subway station and distance to it\n",
    "            s = subway_information[0].text\n",
    "            x = s.split(' miles ')\n",
    "            if len(x) == 1: continue\n",
    "            map[ad_id][\"distance\"] = float(x[0])\n",
    "            map[ad_id][\"subway_station\"] = x[1]\n",
    "\n",
    "    print(\"Scraping task finished\")\n",
    "\n",
    "    #transform to dict to list\n",
    "    result = []\n",
    "    cur_row = 0\n",
    "    for cur_id in map.keys():\n",
    "        link = map[cur_id][\"link\"]\n",
    "        cur_price = map[cur_id][\"price\"]\n",
    "        cur_bedroom = map[cur_id][\"room_nr\"]\n",
    "        cur_bathroom = map[cur_id][\"bath_nr\"]\n",
    "        cur_living = map[cur_id][\"living_nr\"]\n",
    "        cur_address = map[cur_id][\"address\"]\n",
    "        cur_distance = map[cur_id][\"distance\"]\n",
    "        cur_subway_station = map[cur_id][\"subway_station\"]\n",
    "        result.append([])\n",
    "        result[cur_row].append(str(cur_id))\n",
    "        result[cur_row].append(str(link))\n",
    "        result[cur_row].append(str(cur_price))\n",
    "        result[cur_row].append(str(cur_bedroom))\n",
    "        result[cur_row].append(str(cur_bathroom))\n",
    "        result[cur_row].append(str(cur_living))\n",
    "        result[cur_row].append(str(cur_address))\n",
    "        result[cur_row].append(str(cur_distance))\n",
    "        result[cur_row].append(str(cur_subway_station))\n",
    "        cur_row += 1\n",
    "\n",
    "    #transform to dataframe\n",
    "    df = pd.DataFrame(result, columns = [\"ad_id\", \"link\", \"price\", \"bedrooms\", \"bathrooms\", \"living_rooms\", \"address\", \"distance\", \"subway_station\"])\n",
    "    df\n",
    "\n",
    "    #Adjusting \"None values to be NaN for df\n",
    "    df = df.replace(r'None', np.NaN)\n",
    "    # df = df.where(pd.notnull(df), None)\n",
    "    log.info(\"Scraping succesful\")\n",
    "\n",
    "\n",
    "    #Establishing S3 connection\n",
    "    s3 = S3Hook(kwargs['aws_conn_id'])\n",
    "    bucket_name = kwargs['bucket_name']\n",
    "\n",
    "    #creating timestamp\n",
    "\n",
    "    # from datetime import datetime\n",
    "    #\n",
    "    # now = datetime.now() # current date and time\n",
    "\n",
    "    # date_time = now.strftime(\"%Y_%m_%d_%HH_%Mm\")\n",
    "    # print(\"date and time:\",date_time)\n",
    "\n",
    "    #name of the file\n",
    "    key = Variable.get(\"housing_webscraping_get_csv\", deserialize_json=True)['key2']+\".csv\" #using this format as we would like to attempt to use datetime to identify files\n",
    "\n",
    "    # Prepare the file to send to s3\n",
    "    csv_buffer_zoopla = io.StringIO()\n",
    "    #Ensuring the CSV files treats \"NAN\" as null values\n",
    "    zoopla_csv=df.to_csv(csv_buffer_zoopla, index=False)\n",
    "\n",
    "    # Save the pandas dataframe as a csv to s3\n",
    "    s3 = s3.get_resource_type('s3')\n",
    "\n",
    "    # Get the data type object from pandas dataframe, key and connection object to s3 bucket\n",
    "    data = csv_buffer_zoopla.getvalue()\n",
    "\n",
    "    print(\"Saving CSV file\")\n",
    "    object = s3.Object(bucket_name, key)\n",
    "\n",
    "    # Write the file to S3 bucket in specific path defined in key\n",
    "    object.put(Body=data)\n",
    "\n",
    "    log.info('Finished saving the scraped data to s3')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Webscraping dexters\n",
    "def web_scraping_function_dexters(**kwargs):\n",
    "\n",
    "        # Import packages\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import datetime\n",
    "        from bs4 import BeautifulSoup #requires pip install\n",
    "        import requests #requires pip install\n",
    "        import re\n",
    "        import time\n",
    "\n",
    "        import io\n",
    "        # document time\n",
    "        time_started = str(datetime.datetime.now()).replace(\" \",\"_\").replace(\":\",\"-\")[0:19]\n",
    "        ## Define list of subway stations\n",
    "        Underground_lines = ['Bakerloo', 'Central', 'Circle', 'District', 'DLR', 'Hammersmith & City',\n",
    "                         'Jubilee', 'Metropolitan', 'Northern', 'Piccadilly', 'Victoria', 'Waterloo & City']\n",
    "\n",
    "        ## Function to extract characteristics on each ad from the main webpage\n",
    "        def feature_extract(html_text):\n",
    "\n",
    "            soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "            ## Parse for the different divisions within the add\n",
    "\n",
    "            # ads = soup.find_all('div', class_ = 'result-content') #searches for 'div' and is filtered by the CSS-snippet\n",
    "            ads = soup.find_all('li', class_ = 'result item for-sale infinite-item')#searches for 'div' and is filtered by the CSS-snippet\n",
    "            ## Set-up for the loop\n",
    "            results = {} #create nested dictionary to store the results\n",
    "            id_ad = 0 #insert ad_ID to distinguish between each ad\n",
    "\n",
    "            ## Loop across all ads\n",
    "            for k in range(len(ads)):\n",
    "                ad = ads[k]\n",
    "                id_ad += 1\n",
    "                results[id_ad] = {}\n",
    "\n",
    "                ## Extracting features from the ad\n",
    "                name = ad.find('h3').a.contents[0]\n",
    "                try:\n",
    "                    price = ad.find('span', class_ = 'price-qualifier').text #catches the price WITHIN one ad\n",
    "                except:\n",
    "                    continue\n",
    "                address = ad.find('span', class_ = 'address-area-post').text\n",
    "\n",
    "                # Number of bedrooms extracted from string\n",
    "                try:\n",
    "                    bedrooms = ad.find('li', class_ = 'Bedrooms').text\n",
    "                except:\n",
    "                    continue\n",
    "                bedrooms_nbr = int(bedrooms.split()[0])\n",
    "\n",
    "                # Number of bedrooms extracted from string\n",
    "                bathrooms_str = str(ad.find('li',class_ = 'Bathrooms'))\n",
    "                bathrooms_nbr = re.findall(r'\\d+', bathrooms_str)\n",
    "                bathrooms_nbr2 = int(bathrooms_nbr[0] if len(bathrooms_nbr)!= 0  else 0)\n",
    "\n",
    "                # Number of bedrooms extracted from string\n",
    "                reception_str = str(ad.find('li',class_ = 'Receptions'))\n",
    "                reception_nbr = re.findall(r'\\d+', reception_str)\n",
    "                reception_nbr2 = int(reception_nbr[0] if len(reception_nbr)!= 0  else 1)\n",
    "\n",
    "                link = ad.find('h3').a.get(\"href\")\n",
    "\n",
    "                ad_id = ads[k]['data-property-id']\n",
    "\n",
    "                # Create dictionary of results per ad id\n",
    "                results[id_ad]['ad_id'] = ad_id\n",
    "                results[id_ad][\"street_name\"] = name\n",
    "                results[id_ad][\"price\"] = price\n",
    "                results[id_ad][\"address\"] = address\n",
    "                results[id_ad][\"bedrooms\"] = bedrooms_nbr\n",
    "                results[id_ad][\"bathrooms\"] = bathrooms_nbr2\n",
    "                results[id_ad][\"reception\"] = reception_nbr2\n",
    "                results[id_ad][\"link\"] = (\"https://www.dexters.co.uk\" + link)\n",
    "\n",
    "                # Create dataframe from dictionary of results\n",
    "                df_houses = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "            return df_houses\n",
    "\n",
    "        ## Function to create list of pages base on url and number of iterations desired\n",
    "        def page_list(string, iterations):\n",
    "            pages_list = []\n",
    "            for i in range(iterations):\n",
    "                pages_list.append(string + str(i+1))\n",
    "\n",
    "            return pages_list\n",
    "\n",
    "        ## Function to get the maximum number of listing on Dexter's website\n",
    "        def page_max(url):\n",
    "            html_text = requests.get(url).text\n",
    "            soup = BeautifulSoup(html_text, 'lxml')\n",
    "            amount = soup.find('span', class_ = 'marker-count has-results').text\n",
    "            amount_num = re.sub('\\D', '', amount)\n",
    "            return int(amount_num)\n",
    "\n",
    "        ## Function to launch scrapper on a specific webpage with number of pages to scrap\n",
    "        def pages_scrap(main_page, iter_page, pages):\n",
    "            max_pages = (page_max(main_page)/18)\n",
    "            list_of_pages = page_list(iter_page, pages) # Create list of pages to scrape\n",
    "            df_list = [] #Create list of dataframes to be concatenated by the end of the loop\n",
    "\n",
    "            # Loop through all pages to create the different dataframes\n",
    "            for page in list_of_pages:\n",
    "                html_page = requests.get(page)\n",
    "                html_page.encoding = 'utf-8'\n",
    "                page = html_page.text\n",
    "                df_ads = feature_extract(page)\n",
    "                df_list.append(df_ads)\n",
    "\n",
    "            # Concatenate the different dataframes\n",
    "            df_results = pd.concat(df_list)\n",
    "            df_results = df_results.drop_duplicates()\n",
    "            df_results = df_results.reset_index(drop=True)\n",
    "\n",
    "            print('Remaining number of page: ', int(max_pages - pages) )\n",
    "\n",
    "            return df_results\n",
    "        # 1.2 Subway related functions\n",
    "\n",
    "        ## Function to extract subway info list from a house webpage on dexter\n",
    "        def get_info_subway(link):\n",
    "            html_text = requests.get(link).text\n",
    "            soup = BeautifulSoup(html_text, 'lxml')\n",
    "            subway = soup.find('ul', class_ = 'list-information').text\n",
    "\n",
    "            return subway\n",
    "\n",
    "        ## Function to get list of values for subway distances with string\n",
    "        def sub_values(string):\n",
    "            split = string.split('\\n')\n",
    "            list_1 = list(filter(None, split))\n",
    "\n",
    "            list_2 = []\n",
    "            for i in list_1:\n",
    "                x = i.split('-')\n",
    "                list_2.append(x)\n",
    "\n",
    "            list_3 = [item.strip() for sublist in list_2 for item in sublist]\n",
    "            list_4 = list_3[0:3]\n",
    "\n",
    "            return list_3\n",
    "\n",
    "        ## Function to get the closest stop on the tube if any\n",
    "        def closest_line(list_of_lines):\n",
    "            j = 0\n",
    "            nearby_data = []\n",
    "            for i in range(len(list_of_lines)):\n",
    "                if list_of_lines[i] == 'London Underground' or list_of_lines[i] in Underground_lines and (j != 1 and i!=0):\n",
    "                    if (' ' in list_of_lines[i-2]) == False :\n",
    "                        nearby_data.append(list_of_lines[i-3])\n",
    "                        nearby_data.append(list_of_lines[i-2])\n",
    "                        nearby_data.append(list_of_lines[i-1])\n",
    "                        nearby_data.append(list_of_lines[i])\n",
    "                        j = 1\n",
    "\n",
    "                        nearby_data[0] = (' '.join(nearby_data[0:2]))\n",
    "                        del nearby_data[1]\n",
    "\n",
    "                    else:\n",
    "                        nearby_data.append(list_of_lines[i-2])\n",
    "                        nearby_data.append(list_of_lines[i-1])\n",
    "                        nearby_data.append(list_of_lines[i])\n",
    "                        j = 1\n",
    "\n",
    "            return nearby_data\n",
    "\n",
    "        ## Function to populate datafrmae with closest tube stop name, distance, and related tube line\n",
    "        def subway_per_house(df):\n",
    "            #Create new empty (NaN) columns in the existing dataframe\n",
    "            df = df.reindex(columns = df.columns.tolist() + ['subway_station','distance','tube_line'])\n",
    "\n",
    "            #Loop through all lines of dataframe\n",
    "            for i in range(len(df)):\n",
    "                x = df['link'].iloc[i] #Get link of house page to scrape\n",
    "                subs = get_info_subway(x) #Extract tube line info\n",
    "                subs_2 = sub_values(subs) #Get list of subway station and distance\n",
    "                subs_3 = closest_line(subs_2) #Extract closest tube station only\n",
    "\n",
    "                # Populate dataframe if a tubeway station has been found or not\n",
    "                if len(subs_3)!= 0:\n",
    "                    df['subway_station'].iloc[i] = subs_3[0]\n",
    "                    df['distance'].iloc[i] = subs_3[1]\n",
    "                    df['tube_line'].iloc[i] = subs_3[2]\n",
    "                else:\n",
    "                    df['subway_station'].iloc[i] = np.NaN\n",
    "                    df['distance'].iloc[i] = np.NaN\n",
    "                    df['tube_line'].iloc[i] = np.NaN\n",
    "\n",
    "            df = df.astype(str)\n",
    "\n",
    "            return df\n",
    "\n",
    "        #Functions to clean subway output\n",
    "        def get_tube_dist(string):\n",
    "            string_m = string.split(' ')\n",
    "            num_val = string_m[-1]\n",
    "\n",
    "            return num_val\n",
    "        def strip_tube(string):\n",
    "            string_m = string.split(' ')\n",
    "            string_m = string_m[:-1]\n",
    "            string_m = ' '.join(string_m)\n",
    "\n",
    "            return string_m\n",
    "        def hasNumbers(inputString):\n",
    "            return any(char.isdigit() for char in inputString)\n",
    "\n",
    "        ## Function to clean subway stops when too many words in the string\n",
    "        def clean_tube_stop_string(string):\n",
    "            forbiddden_words = ['London Overground', 'Railway', 'Network Rail', 'Tramlink']\n",
    "            count_forbidden = 0\n",
    "\n",
    "            for j in forbiddden_words:\n",
    "                if count_forbidden == 0:\n",
    "                    if j in string:\n",
    "                        string_update = string.split()[-1]\n",
    "                        count_forbidden = 1\n",
    "                    else:\n",
    "                        string_update = string\n",
    "\n",
    "            return(string_update)\n",
    "\n",
    "        ## Function to input tube distance into the right column when value is in 'tube_stop'\n",
    "        def clean_tube_dist(df):\n",
    "            df['distance'] = df['distance'].astype('str')\n",
    "\n",
    "            errors  = df[df.loc[:, 'distance'].map(hasNumbers) == False].copy()\n",
    "            errors_2 = errors.loc[errors['subway_station'] != 'NaN'].copy()\n",
    "            errors_2.loc[:, 'distance'] = errors_2.loc[:, 'subway_station'].map(get_tube_dist)\n",
    "            errors_2.loc[:, 'subway_station'] = errors_2.loc[:, 'subway_station'].map(strip_tube)\n",
    "            errors_2\n",
    "\n",
    "            #Create copy of original df for modification\n",
    "            df_copy = df.copy()\n",
    "\n",
    "            # replace values in final df\n",
    "            for i in errors_2.index:\n",
    "                df_copy.loc[i] = errors_2.loc[i]\n",
    "\n",
    "            return df_copy\n",
    "\n",
    "        ## Functions to deal with Victoria tube stops (Victoria being both a tube stop and a tube line)\n",
    "        def victoria_clean_stop(string):\n",
    "            str_vic = 'Victoria'\n",
    "            str_check = string.split()\n",
    "            if str_check[0] == 'Victoria':\n",
    "                str_return = str_check[1]\n",
    "            else:\n",
    "                str_return = str_vic\n",
    "\n",
    "            return str_return\n",
    "        def clean_tube_victoria(df):\n",
    "            df['subway_station'] = df['subway_station'].astype('str')\n",
    "\n",
    "            errors  = df[df['subway_station'].str.contains('Victoria')].copy()\n",
    "\n",
    "            errors.loc[:, 'subway_station'] = errors.loc[:, 'subway_station'].map(victoria_clean_stop)\n",
    "\n",
    "            #Create copy of original df for modification\n",
    "            df_copy = df.copy()\n",
    "\n",
    "            # Replace values in final df\n",
    "            for i in errors.index:\n",
    "                df_copy.loc[i] = errors.loc[i]\n",
    "\n",
    "            return df_copy\n",
    "\n",
    "        ## Final cleaning function to apply previous cleaning on 'tube_stop' and 'tube_dist' for the whole dataframe\n",
    "        def clean_tube_stop(df):\n",
    "            df_2 = df.copy()\n",
    "            df_2 = clean_tube_dist(df_2)\n",
    "            df_2['subway_station'] = df_2['subway_station'].astype('str')\n",
    "            df_2['subway_station'] = df_2['subway_station'].map(clean_tube_stop_string)\n",
    "\n",
    "            df_2 = clean_tube_victoria(df_2)\n",
    "            # #Keep the ID of the add as index or not\n",
    "\n",
    "\n",
    "            return df_2\n",
    "\n",
    "        dexters_list_1 = pages_scrap('https://www.dexters.co.uk/property-sales/properties-for-sale-in-london',\n",
    "                                    'https://www.dexters.co.uk/property-sales/properties-for-sale-in-london/page-', 50)\n",
    "\n",
    "\n",
    "        ## Fetch subway related information from the previous dataframe\n",
    "        output_list = subway_per_house(dexters_list_1)\n",
    "\n",
    "        output_list\n",
    "\n",
    "        cleaned = clean_tube_stop(output_list)\n",
    "        cleaned\n",
    "\n",
    "        #Cleaning the price and distance variables and converting to float\n",
    "        cleaned['price'] = cleaned['price'].str.replace('Â£', '')\n",
    "        cleaned['price'] = cleaned['price'].str.replace(',', '').astype(float)\n",
    "        cleaned['distance'] = cleaned['distance'].str.replace('m', '').astype(float)\n",
    "\n",
    "\n",
    "        cleaned['subway_station'].nunique()\n",
    "        cleaned_dict = cleaned.to_dict(orient='records')\n",
    "\n",
    "        log.info('Finished scraping the data')\n",
    "        #create connection for uploading the file to S3\n",
    "\n",
    "        #Establishing S3 connection\n",
    "        s3 = S3Hook(kwargs['aws_conn_id'])\n",
    "        bucket_name = kwargs['bucket_name']\n",
    "        #name of the file\n",
    "        key = Variable.get(\"housing_webscraping_get_csv\", deserialize_json=True)['key1']+\".csv\"\n",
    "\n",
    "        # Prepare the file to send to s3\n",
    "        csv_buffer = io.StringIO()\n",
    "        #Ensuring the CSV files treats \"NAN\" as null values\n",
    "        cleaned_csv=cleaned.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        # Save the pandas dataframe as a csv to s3\n",
    "        s3 = s3.get_resource_type('s3')\n",
    "\n",
    "        # Get the data type object from pandas dataframe, key and connection object to s3 bucket\n",
    "        data = csv_buffer.getvalue()\n",
    "\n",
    "        print(\"Saving Parquet file\")\n",
    "        object = s3.Object(bucket_name, key)\n",
    "\n",
    "        # Write the file to S3 bucket in specific path defined in key\n",
    "        object.put(Body=data)\n",
    "\n",
    "        log.info('Finished saving the scraped data to s3')\n",
    "\n",
    "\n",
    "        return cleaned_dict\n",
    "\n",
    "\n",
    "# Webscraping London Air Quality\n",
    "def web_scraping_function_londonair(**kwargs):\n",
    "\n",
    "        # Import packages\n",
    "        ## import liabrairies\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import re\n",
    "        import requests\n",
    "        import json\n",
    "        import io\n",
    "\n",
    "\n",
    "        #######################\n",
    "        ## 1. Define sites to query\n",
    "        #######################\n",
    "        base_api = 'https://api.erg.ic.ac.uk/AirQuality'\n",
    "\n",
    "\n",
    "        sites_to_query = ['TH4', 'BQ7', 'EA8', 'EI8', 'EI3',\n",
    "                          'CR9', 'GB6', 'CT3', 'HG4', 'EA6',\n",
    "                          'HI0', 'ST9', 'CR5', 'CT2', 'EN4',\n",
    "                          'EN5', 'IS6', 'HP1', 'MY1', 'WAB',\n",
    "                          'CT8', 'ST6', 'CT4', 'RB7', 'BX1',\n",
    "                          'RI1', 'WA9', 'WAA', 'KC1', 'LW1',\n",
    "                          'GN0', 'BG2', 'ST5', 'ME9', 'CD1',\n",
    "                          'BX2', 'SKA', 'KT4', 'TH2', 'BY7',\n",
    "                          'LW4', 'GV2', 'CT6', 'BT8', 'HV3',\n",
    "                          'EN7', 'LB4', 'LW2', 'HG1', 'RB4',\n",
    "                          'HR1', 'EN1', 'HR2', 'IS2', 'WMB',\n",
    "                          'WMC', 'ST4', 'HV1', 'IM1', 'WM5',\n",
    "                          'GR8', 'GN4', 'WAC', 'BT5', 'ME2',\n",
    "                          'GR7', 'BG1', 'HK6', 'LW5', 'WA2',\n",
    "                          'WA7', 'SK6', 'GR9', 'BL0', 'LH0',\n",
    "                          'TD5', 'LB6', 'GV1', 'RI2', 'KT5',\n",
    "                          'BT6', 'GN5', 'WM6', 'CR8', 'KT6',\n",
    "                          'LB5', 'EI1', 'GN6', 'SK5', 'WM0',\n",
    "                          'CR7', 'BT4', 'NB1', 'WMD']\n",
    "\n",
    "        #######################\n",
    "        ## 2. Defines functions\n",
    "        #######################\n",
    "        def request_json(link):\n",
    "            response = requests.get(link)\n",
    "            rep_json = response.text\n",
    "            parsed = json.loads(rep_json)\n",
    "            return parsed\n",
    "\n",
    "        def request_json_2(link):\n",
    "            # Get json response for the group\n",
    "            site_request = (link)\n",
    "            link_info = base_api + site_request\n",
    "            info_json = request_json(link_info)\n",
    "\n",
    "            return info_json\n",
    "        ## Function to read measures given by the API\n",
    "        def get_measures(json):\n",
    "\n",
    "            # Create empty lists for storage\n",
    "            SpeciesCode = []\n",
    "            MeasurementDateGMT = []\n",
    "            Value = []\n",
    "\n",
    "            # Loop through the json file\n",
    "            for i in range(len(json['AirQualityData']['Data'])):\n",
    "                root_data = json['AirQualityData']['Data']\n",
    "                SpeciesCode.append(root_data[i]['@SpeciesCode'])\n",
    "                MeasurementDateGMT.append(root_data[i]['@MeasurementDateGMT'])\n",
    "                Value.append(root_data[i]['@Value'])\n",
    "\n",
    "            # Create final dataframe\n",
    "            df_measures = pd.DataFrame(np.column_stack([SpeciesCode, MeasurementDateGMT, Value]),\n",
    "                                            columns=['species_code', 'measurement_date_gmt', 'value'])\n",
    "\n",
    "            # Formatting - insert side code, rearrange columns and replace empty strings\n",
    "            df_measures['site_code'] = json['AirQualityData']['@SiteCode']\n",
    "            df_measures = df_measures[['site_code', 'measurement_date_gmt', 'species_code', 'value']]\n",
    "            df_measures['value'] = df_measures['value'].replace('', 0)\n",
    "\n",
    "            return df_measures\n",
    "\n",
    "        ## Function to get measures for all sites between a certain timeframe\n",
    "        def get_record(sites, start_date, end_date):\n",
    "\n",
    "            #Create empty dataframe\n",
    "            df_record = pd.DataFrame()\n",
    "\n",
    "            # Loop through list of sites\n",
    "            for i in sites:\n",
    "                site_request = ('/Data/Site/SiteCode=' + i + '/StartDate=' + start_date + '/EndDate=' + end_date + '/Json')\n",
    "                link_request = base_api + site_request\n",
    "                json_response = request_json(link_request)\n",
    "                df_site = get_measures(json_response)\n",
    "                df_record = df_record.append(df_site)\n",
    "\n",
    "            return df_record\n",
    "\n",
    "        #######################\n",
    "        ## 3. Set parameters for the query\n",
    "        #######################\n",
    "\n",
    "        ## Query LondonAir API for all sites closed to the subway stations, for the past 7 days\n",
    "\n",
    "        from datetime import datetime, timedelta\n",
    "\n",
    "        end = datetime.today()\n",
    "        end_date = end.strftime('%Y-%m-%d')\n",
    "\n",
    "        start = end - timedelta(days=1)\n",
    "        start_date = start.strftime('%Y-%m-%d')\n",
    "\n",
    "        site_list = sites_to_query\n",
    "\n",
    "        #######################\n",
    "        ## 4. Execute request\n",
    "        #######################\n",
    "\n",
    "        record_1 = get_record(site_list, start_date, end_date)\n",
    "        record_1 = record_1.reset_index(drop = True)\n",
    "\n",
    "        londonair_dict = record_1.to_dict(orient='records')\n",
    "\n",
    "        log.info('Finished scraping the data')\n",
    "        #create connection for uploading the file to S3\n",
    "\n",
    "        #Establishing S3 connection\n",
    "        s3 = S3Hook(kwargs['aws_conn_id'])\n",
    "        bucket_name = kwargs['bucket_name']\n",
    "        #name of the file\n",
    "        key = Variable.get(\"housing_webscraping_get_csv\", deserialize_json=True)['key4']+\".csv\"\n",
    "\n",
    "        # Prepare the file to send to s3\n",
    "        csv_buffer = io.StringIO()\n",
    "        #Ensuring the CSV files treats \"NAN\" as null values\n",
    "        londonair_csv=record_1.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        # Save the pandas dataframe as a csv to s3\n",
    "        s3 = s3.get_resource_type('s3')\n",
    "\n",
    "        # Get the data type object from pandas dataframe, key and connection object to s3 bucket\n",
    "        data = csv_buffer.getvalue()\n",
    "\n",
    "        print(\"Saving Parquet file\")\n",
    "        object = s3.Object(bucket_name, key)\n",
    "\n",
    "        # Write the file to S3 bucket in specific path defined in key\n",
    "        object.put(Body=data)\n",
    "\n",
    "        log.info('Finished saving the scraped data to s3')\n",
    "\n",
    "\n",
    "        return londonair_dict\n",
    "\n",
    "# Saving file to postgreSQL database\n",
    "def save_result_to_postgres_db_zoopla(**kwargs):\n",
    "\n",
    "    import pandas as pd\n",
    "    import io\n",
    "\n",
    "    #Establishing connection to S3 bucket\n",
    "    bucket_name = kwargs['bucket_name']\n",
    "    key = Variable.get(\"housing_webscraping_get_csv\", deserialize_json=True)['key2']+\".csv\"\n",
    "    s3 = S3Hook(kwargs['aws_conn_id'])\n",
    "    log.info(\"Established connection to S3 bucket\")\n",
    "\n",
    "\n",
    "    # Get the task instance\n",
    "    task_instance = kwargs['ti']\n",
    "    print(task_instance)\n",
    "\n",
    "\n",
    "    # Read the content of the key from the bucket\n",
    "    csv_bytes_zoopla = s3.read_key(key, bucket_name)\n",
    "    # Read the CSV\n",
    "    clean_zoopla = pd.read_csv(io.StringIO(csv_bytes_zoopla ))#, encoding='utf-8')\n",
    "\n",
    "    log.info('passing Zoopla data from S3 bucket')\n",
    "\n",
    "    # Connect to the PostgreSQL database\n",
    "    pg_hook = PostgresHook(postgres_conn_id=kwargs[\"postgres_conn_id\"], schema=kwargs['db_name'])\n",
    "    conn = pg_hook.get_conn()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    log.info('Initialised connection')\n",
    "\n",
    "    #Required code for clearing an error related to int64\n",
    "    import numpy\n",
    "    from psycopg2.extensions import register_adapter, AsIs\n",
    "    def addapt_numpy_float64(numpy_float64):\n",
    "        return AsIs(numpy_float64)\n",
    "    def addapt_numpy_int64(numpy_int64):\n",
    "        return AsIs(numpy_int64)\n",
    "    register_adapter(numpy.float64, addapt_numpy_float64)\n",
    "    register_adapter(numpy.int64, addapt_numpy_int64)\n",
    "\n",
    "    log.info('Loading row by row into database')\n",
    "    # #Removing NaN values and converting to NULL:\n",
    "\n",
    "    clean_zoopla = clean_zoopla.where(pd.notnull(clean_zoopla), None)\n",
    "\n",
    "    s = \"\"\"INSERT INTO schema_housing.zoopla( ad_id, link, price, bedrooms, bathrooms, living_rooms, address, distance, subway_station) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    for index in range(len(clean_zoopla)):\n",
    "        obj = []\n",
    "\n",
    "        obj.append([clean_zoopla.ad_id[index],\n",
    "                   clean_zoopla.link[index],\n",
    "                   clean_zoopla.price[index],\n",
    "                   clean_zoopla.bedrooms[index],\n",
    "                   clean_zoopla.bathrooms[index],\n",
    "                   clean_zoopla.living_rooms[index],\n",
    "                   clean_zoopla.address[index],\n",
    "                   clean_zoopla.distance[index],\n",
    "                   clean_zoopla.subway_station[index]])\n",
    "\n",
    "        cursor.executemany(s, obj)\n",
    "        conn.commit()\n",
    "\n",
    "    log.info('Finished saving the scraped data to postgres database')\n",
    "\n",
    "\n",
    "# Saving Dexter file to postgreSQL database\n",
    "def save_result_to_postgres_db_dexters(**kwargs):\n",
    "\n",
    "    import pandas as pd\n",
    "    import io\n",
    "\n",
    "    #Establishing connection to S3 bucket\n",
    "    bucket_name = kwargs['bucket_name']\n",
    "    key = Variable.get(\"housing_webscraping_get_csv\", deserialize_json=True)['key1']+\".csv\"\n",
    "    s3 = S3Hook(kwargs['aws_conn_id'])\n",
    "    log.info(\"Established connection to S3 bucket\")\n",
    "\n",
    "\n",
    "    # Get the task instance\n",
    "    task_instance = kwargs['ti']\n",
    "    print(task_instance)\n",
    "\n",
    "\n",
    "    # Read the content of the key from the bucket\n",
    "    csv_bytes = s3.read_key(key, bucket_name)\n",
    "    # Read the CSV\n",
    "    clean_dexters = pd.read_csv(io.StringIO(csv_bytes ))#, encoding='utf-8')\n",
    "\n",
    "    log.info('passing dexters data from S3 bucket')\n",
    "\n",
    "    # Connect to the PostgreSQL database\n",
    "    pg_hook = PostgresHook(postgres_conn_id=kwargs[\"postgres_conn_id\"], schema=kwargs['db_name'])\n",
    "    conn = pg_hook.get_conn()\n",
    "    cursor = conn.cursor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
