{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "from airflow.models import Variable\n",
    "from airflow.hooks.S3_hook import S3Hook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Set up the main configurations of the dag\n",
    "# =============================================================================\n",
    "# now = datetime.now() # current date and time\n",
    "# date_time = now.strftime(\"%Y_%m_%d_%HH\")\n",
    "# print(\"date and time:\",date_time)\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2021, 3, 8),\n",
    "    'owner': 'Airflow',\n",
    "    'filestore_base': '/tmp/airflowtemp/',\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'aws_conn_id': \"aws_default_starthackathon\",\n",
    "    'bucket_name': Variable.get(\"ml_pipeline\", deserialize_json=True)['bucket_name'],\n",
    "    'postgres_conn_id': 'starthackathon_christopherkindl',\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'db_name': Variable.get(\"housing_db\", deserialize_json=True)['db_name']\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG('ml_pipeline',\n",
    "          description='machine learning pipeling for start hackathon 2021',\n",
    "          schedule_interval='@weekly',\n",
    "          catchup=False,\n",
    "          default_args=default_args,\n",
    "          max_active_runs=1)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Define different functions\n",
    "# =============================================================================\n",
    "\n",
    "# Creating schema if inexistant\n",
    "def create_schema(**kwargs):\n",
    "    pg_hook = PostgresHook(postgres_conn_id=kwargs['postgres_conn_id'], schema=kwargs['db_name'])\n",
    "    conn = pg_hook.get_conn()\n",
    "    cursor = conn.cursor()\n",
    "    log.info('Initialised connection')\n",
    "    sql_queries = \"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS sbb_schema;\n",
    "    DROP TABLE IF EXISTS sbb_schema.prediction;\n",
    "    CREATE TABLE IF NOT EXISTS sbb_schema.prediction(\n",
    "        \"id\" numeric,\n",
    "        \"occupancy_rate\" numeric,\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(sql_queries)\n",
    "    conn.commit()\n",
    "    log.info(\"Created Schema and Tables\")\n",
    "\n",
    "def ml_train_prediction(**kwargs):\n",
    "    \n",
    "    # Machine Learning Pipeline\n",
    "    list1 = [i for i in range(20)]\n",
    "    list2 = [i**2 for i in range(20)]\n",
    "\n",
    "    log.info(\"Train data creation successfull\")\n",
    "\n",
    "    df = pd.DataFrame(list(zip(list1, list1)), columns=['list1','list2'])\n",
    "\n",
    "    log.info(\"Prediction done\")\n",
    "\n",
    "    #Establishing S3 connection\n",
    "    s3 = S3Hook(kwargs['aws_conn_id'])\n",
    "    bucket_name = kwargs['bucket_name']\n",
    "\n",
    "    #creating timestamp\n",
    "\n",
    "    # from datetime import datetime\n",
    "    #\n",
    "    # now = datetime.now() # current date and time\n",
    "\n",
    "    # date_time = now.strftime(\"%Y_%m_%d_%HH_%Mm\")\n",
    "    # print(\"date and time:\",date_time)\n",
    "\n",
    "    #name of the file\n",
    "    key = Variable.get(\"get_csv\", deserialize_json=True)['key1']+\".csv\" \n",
    "\n",
    "    # Prepare the file to send to s3\n",
    "    #csv_buffer_zoopla = io.StringIO()\n",
    "    csv = io.StringIO()\n",
    "    \n",
    "    #Ensuring the CSV files treats \"NAN\" as null values\n",
    "    #zoopla_csv=df.to_csv(csv, index=False)\n",
    "    zoopla_csv=df.to_csv(csv, index=False)\n",
    "\n",
    "    # Save the pandas dataframe as a csv to s3\n",
    "    s3 = s3.get_resource_type('s3')\n",
    "\n",
    "    # Get the data type object from pandas dataframe, key and connection object to s3 bucket\n",
    "    #data = csv_buffer_zoopla.getvalue()\n",
    "    data = csv.getvalue()\n",
    "\n",
    "    log.info('saving file')\n",
    "    object = s3.Object(bucket_name, key)\n",
    "\n",
    "    # Write the file to S3 bucket in specific path defined in key\n",
    "    object.put(Body=data)\n",
    "\n",
    "    log.info('Finished saving the scraped data to s3')\n",
    "    return\n",
    "\n",
    "## DUMMMY TEST\n",
    "    \n",
    "# Saving file to postgreSQL database\n",
    "def save_result_to_postgres_db(**kwargs):\n",
    "\n",
    "    import pandas as pd\n",
    "    import io\n",
    "\n",
    "    #Establishing connection to S3 bucket\n",
    "    bucket_name = kwargs['bucket_name']\n",
    "    key = Variable.get(\"get_csv\", deserialize_json=True)['key2']+\".csv\"\n",
    "    s3 = S3Hook(kwargs['aws_conn_id'])\n",
    "    log.info(\"Established connection to S3 bucket\")\n",
    "\n",
    "\n",
    "    # Get the task instance\n",
    "    task_instance = kwargs['ti']\n",
    "    print(task_instance)\n",
    "\n",
    "\n",
    "    # Read the content of the key from the bucket\n",
    "    #csv_bytes_zoopla = s3.read_key(key, bucket_name)\n",
    "    prediction = s3.read_key(key, bucket_name)\n",
    "    \n",
    "    # Read the CSV\n",
    "    #clean_zoopla = pd.read_csv(io.StringIO(prediction))#, encoding='utf-8')\n",
    "    prediction_csv = pd.read_csv(io.StringIO(prediction))#, encoding='utf-8')\n",
    "\n",
    "    log.info('passing data from S3 bucket')\n",
    "\n",
    "    # Connect to the PostgreSQL database\n",
    "    pg_hook = PostgresHook(postgres_conn_id=kwargs[\"postgres_conn_id\"], schema=kwargs['db_name'])\n",
    "    conn = pg_hook.get_conn()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    log.info('Initialised connection')\n",
    "\n",
    "    #Required code for clearing an error related to int64\n",
    "    import numpy\n",
    "    from psycopg2.extensions import register_adapter, AsIs\n",
    "    def addapt_numpy_float64(numpy_float64):\n",
    "        return AsIs(numpy_float64)\n",
    "    def addapt_numpy_int64(numpy_int64):\n",
    "        return AsIs(numpy_int64)\n",
    "    register_adapter(numpy.float64, addapt_numpy_float64)\n",
    "    register_adapter(numpy.int64, addapt_numpy_int64)\n",
    "\n",
    "    log.info('Loading row by row into database')\n",
    "    # #Removing NaN values and converting to NULL:\n",
    "\n",
    "    #clean_zoopla = clean_zoopla.where(pd.notnull(clean_zoopla), None)\n",
    "\n",
    "    s = \"\"\"INSERT INTO sbb_schema.prediction(id, occupancy_rate) VALUES (%s, %s)\"\"\"\n",
    "    for index in range(len(prediction_csv)):\n",
    "        obj = []\n",
    "\n",
    "        obj.append([prediction_csv.id[index],\n",
    "                   prediction_csv.occupancy_rate[index]])\n",
    "\n",
    "        cursor.executemany(s, obj)\n",
    "        conn.commit()\n",
    "\n",
    "    log.info('Finished saving the to postgres database')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Set up the main configurations of the dag\n",
    "# =============================================================================\n",
    "\n",
    "create_schema = PythonOperator(\n",
    "    task_id='create_schema',\n",
    "    python_callable=create_schema,\n",
    "    op_kwargs=default_args,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "web_scraping_task_zoopla = PythonOperator(\n",
    "    task_id='ml_task',\n",
    "    provide_context=True,\n",
    "    python_callable=ml_train_prediction,\n",
    "    op_kwargs=default_args,\n",
    "    dag=dag,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "save_result_to_postgres_db = PythonOperator(\n",
    "    task_id='save_result_to_postgres_db',\n",
    "    provide_context=True,\n",
    "    python_callable=save_result_to_postgres_db,\n",
    "    trigger_rule=TriggerRule.ALL_SUCCESS,\n",
    "    op_kwargs=default_args,\n",
    "    dag=dag,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# =============================================================================\n",
    "# 4. Indicating the order of the dags\n",
    "# =============================================================================\n",
    "\n",
    "create_schema >> ml_train_prediction >> save_result_to_postgres_db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
